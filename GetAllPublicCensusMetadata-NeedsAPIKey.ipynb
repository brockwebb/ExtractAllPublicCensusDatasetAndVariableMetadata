{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181f62b8-4782-4b58-b914-3ef95a59a3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total datasets found: 1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Metadata: 100%|█████████████████████████████████████████| 1636/1636 [00:00<00:00, 216545.11dataset/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets metadata has been saved to 'datasets_metadata.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Variables Metadata: 100%|███████████████████████████████████████████| 1636/1636 [57:06<00:00,  2.09s/dataset]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 1980s has been saved to 'census_metadata_1980s.csv'\n",
      "Metadata for 1990s has been saved to 'census_metadata_1990s.csv'\n",
      "Metadata for 2000s has been saved to 'census_metadata_2000s.csv'\n",
      "Metadata for 2010s has been saved to 'census_metadata_2010s.csv'\n",
      "Metadata for 2020s has been saved to 'census_metadata_2020s.csv'\n",
      "Metadata for Unknown has been saved to 'census_metadata_Unknown.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "# from tqdm.notebook import tqdm  # Remove notebook version\n",
    "from tqdm import tqdm  # Use standard tqdm\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='census_metadata_errors.log', level=logging.DEBUG,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# Set your Census API key\n",
    "API_KEY = 'YOUR API KEY HERE'\n",
    "\n",
    "# Function to determine the decade\n",
    "def get_decade(year):\n",
    "    try:\n",
    "        return f\"{(int(year) // 10) * 10}s\"\n",
    "    except ValueError:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Function to extract the year from a string\n",
    "def extract_year(text):\n",
    "    match = re.search(r'(19|20)\\d{2}', text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Set up a session with retries\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=0.1,\n",
    "                status_forcelist=[500, 502, 503, 504], raise_on_status=False)\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# API Discovery endpoint\n",
    "discovery_url = 'https://api.census.gov/data.json'\n",
    "\n",
    "# Make the GET request to retrieve datasets\n",
    "try:\n",
    "    response = session.get(discovery_url, timeout=30)\n",
    "    datasets_json = response.json()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to retrieve datasets: {e}\")\n",
    "    raise SystemExit(f\"Failed to retrieve datasets: {e}\")\n",
    "\n",
    "# Extract the list of datasets\n",
    "datasets = datasets_json.get('dataset', [])\n",
    "if not datasets:\n",
    "    logging.error(\"No datasets found in the API response.\")\n",
    "    raise SystemExit(\"No datasets found in the API response.\")\n",
    "\n",
    "print(f\"Total datasets found: {len(datasets)}\")\n",
    "\n",
    "# Initialize lists to store metadata\n",
    "all_metadata = []\n",
    "datasets_metadata = []\n",
    "variables_links = []\n",
    "\n",
    "# For progress tracking over datasets\n",
    "dataset_iter = tqdm(datasets, desc='Extracting Dataset Metadata', unit='dataset')\n",
    "\n",
    "for dataset in dataset_iter:\n",
    "    # Extract relevant metadata fields\n",
    "    title = dataset.get('title', '')\n",
    "    description = dataset.get('description', '')\n",
    "    identifier = dataset.get('identifier', '')\n",
    "    contact = dataset.get('contactPoint', {}).get('fn', '')\n",
    "    access_level = dataset.get('accessLevel', '')\n",
    "    modified = dataset.get('modified', '')\n",
    "    publisher = dataset.get('publisher', {}).get('name', '')\n",
    "    references = dataset.get('references', [])\n",
    "    keywords = dataset.get('keyword', [])\n",
    "    c_dataset = dataset.get('c_dataset', [])\n",
    "    c_vintage = dataset.get('c_vintage', [])\n",
    "    c_variablesLink = dataset.get('c_variablesLink', '')\n",
    "\n",
    "    # Ensure 'c_dataset' and 'c_vintage' are lists\n",
    "    if not isinstance(c_dataset, list):\n",
    "        c_dataset = [c_dataset]\n",
    "    if not isinstance(c_vintage, list):\n",
    "        c_vintage = [c_vintage]\n",
    "\n",
    "    # For each combination of 'c_dataset' and 'c_vintage', create an entry\n",
    "    for dataset_name in c_dataset:\n",
    "        for year in c_vintage:\n",
    "            dataset_entry = {\n",
    "                'dataset_name': dataset_name,\n",
    "                'year': year,\n",
    "                'title': title,\n",
    "                'description': description,\n",
    "                'identifier': identifier,\n",
    "                'contact': contact,\n",
    "                'access_level': access_level,\n",
    "                'modified': modified,\n",
    "                'publisher': publisher,\n",
    "                'references': ', '.join(references) if references else '',\n",
    "                'keywords': ', '.join(keywords) if keywords else ''\n",
    "            }\n",
    "            datasets_metadata.append(dataset_entry)\n",
    "\n",
    "    # Collect 'c_variablesLink' for variables metadata retrieval\n",
    "    if c_variablesLink:\n",
    "        # If 'c_variablesLink' is a list, extend the variables_links list\n",
    "        if isinstance(c_variablesLink, list):\n",
    "            variables_links.extend([(link, title) for link in c_variablesLink])\n",
    "        else:\n",
    "            variables_links.append((c_variablesLink, title))\n",
    "\n",
    "# Create a DataFrame from the datasets metadata\n",
    "datasets_metadata_df = pd.DataFrame(datasets_metadata)\n",
    "\n",
    "# Save datasets metadata to CSV\n",
    "datasets_metadata_df.to_csv('datasets_metadata.csv', index=False)\n",
    "\n",
    "print(\"Datasets metadata has been saved to 'datasets_metadata.csv'\")\n",
    "\n",
    "# Now, process the variables links\n",
    "variables_iter = tqdm(variables_links, desc='Processing Variables Metadata', unit='dataset')\n",
    "\n",
    "for variables_url, title in variables_iter:\n",
    "    # Append the API key if needed\n",
    "    if '?' in variables_url:\n",
    "        variables_url_with_key = f'{variables_url}&key={API_KEY}'\n",
    "    else:\n",
    "        variables_url_with_key = f'{variables_url}?key={API_KEY}'\n",
    "\n",
    "    # Extract dataset_name and year from the URL\n",
    "    # Updated regex to handle more complex URL patterns\n",
    "    match = re.match(r'.*/data/((?:19|20)\\d{2})/([^/]+(?:/[^/]+)*)/variables.json', variables_url)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        dataset_name = match.group(2)\n",
    "    else:\n",
    "        # If we can't extract, attempt to extract year from the title\n",
    "        year = extract_year(title)\n",
    "        dataset_name = variables_url\n",
    "\n",
    "    logging.debug(f\"Processing variables URL: {variables_url_with_key}\")\n",
    "    logging.debug(f\"Dataset Name: {dataset_name}, Year: {year}, Title: {title}\")\n",
    "\n",
    "    try:\n",
    "        variables_response = session.get(variables_url_with_key, timeout=30)\n",
    "        content_type = variables_response.headers.get('Content-Type', '')\n",
    "\n",
    "        if variables_response.status_code == 200 and 'application/json' in content_type.lower():\n",
    "            try:\n",
    "                variables_json = variables_response.json()\n",
    "                variables = variables_json.get('variables', {})\n",
    "\n",
    "                for var_name, var_info in variables.items():\n",
    "                    metadata_entry = {\n",
    "                        'dataset_name': dataset_name,\n",
    "                        'year': year,\n",
    "                        'title': title,\n",
    "                        'variable_name': var_name,\n",
    "                        'label': var_info.get('label', ''),\n",
    "                        'concept': var_info.get('concept', ''),\n",
    "                        'predicateType': var_info.get('predicateType', ''),\n",
    "                        'group': var_info.get('group', ''),\n",
    "                        'limit': var_info.get('limit', ''),\n",
    "                        'attributes': var_info.get('attributes', '')\n",
    "                    }\n",
    "                    all_metadata.append(metadata_entry)\n",
    "                logging.info(f\"Successfully processed variables for dataset '{dataset_name}', year '{year}'.\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"JSON decode error for variables URL '{variables_url}': {e}\")\n",
    "                logging.error(f\"Response content: {variables_response.text}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to retrieve valid JSON for variables URL '{variables_url}': HTTP {variables_response.status_code}\")\n",
    "            logging.error(f\"Content-Type: {content_type}\")\n",
    "            logging.error(f\"Response content: {variables_response.text}\")\n",
    "    except requests.Timeout as e:\n",
    "        logging.error(f\"Timeout error for variables URL '{variables_url}': {e}\")\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Request error for variables URL '{variables_url}': {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error for variables URL '{variables_url}': {e}\")\n",
    "        continue\n",
    "\n",
    "    # Respectful delay to avoid hitting rate limits\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Check if any metadata was collected\n",
    "if all_metadata:\n",
    "    # Create a DataFrame from the metadata list\n",
    "    metadata_df = pd.DataFrame(all_metadata)\n",
    "\n",
    "    # Convert 'year' to string if it's not already\n",
    "    metadata_df['year'] = metadata_df['year'].astype(str)\n",
    "    metadata_df['decade'] = metadata_df['year'].apply(get_decade)\n",
    "\n",
    "    # Save metadata to separate CSV files by decade\n",
    "    for decade, group_df in metadata_df.groupby('decade'):\n",
    "        filename = f'census_metadata_{decade}.csv'\n",
    "        group_df.to_csv(filename, index=False)\n",
    "        print(f\"Metadata for {decade} has been saved to '{filename}'\")\n",
    "else:\n",
    "    print(\"No metadata was collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e9fe3e-ac64-47c5-887b-ca5732af0cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
