{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd5e520-1fe4-462b-b78f-326cec887c9e",
   "metadata": {},
   "source": [
    "# 1. Install libraries a/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8bf71a7c-5626-488a-bc01-2d15112e4184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules and dependencies loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (uncomment and run if not already installed)\n",
    "# !pip install pandas neo4j\n",
    "# !pip install beautifulsoup4 lxml \n",
    "# !pip install requests tqdm\n",
    "# !pip install html5lib\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Dynamically add the src directory to sys.path\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"../src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Now, import the modules\n",
    "from web_data_extraction import create_robust_session, fetch_url\n",
    "from progress_tracker import track_progress\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "print(\"Modules and dependencies loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c488fd2-68b3-47e5-90bf-82f5ba2a6f5c",
   "metadata": {},
   "source": [
    "## 2. Create Table of all unique survey IDs\n",
    "- The first step is to create our survey table\n",
    "- This will be the 'Central Table' for organizing all of our information\n",
    "\n",
    "### 2.1 Loading Census API Dataset info\n",
    "- File was obtained from `https://api.census.gov/data.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "216e967d-0a05-4a9a-893d-d148565c50e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables found: 1\n",
      "Headers: ['Title', 'Description', 'Vintage', 'Dataset Name', 'Dataset Type', 'Geography List', 'Variable List', 'Group List', 'SortList', 'Examples', 'Developer Documentation', 'API Base URL']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Dataset Type</th>\n",
       "      <th>Geography List</th>\n",
       "      <th>Variable List</th>\n",
       "      <th>Group List</th>\n",
       "      <th>SortList</th>\n",
       "      <th>Examples</th>\n",
       "      <th>Developer Documentation</th>\n",
       "      <th>API Base URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1648 datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986 County Business Patterns: Business Patterns</td>\n",
       "      <td>County Business Patterns (CBP) is an annual se...</td>\n",
       "      <td>1986</td>\n",
       "      <td>cbp</td>\n",
       "      <td>Aggregate</td>\n",
       "      <td>http://api.census.gov/data/1986/cbp/geography....</td>\n",
       "      <td>http://api.census.gov/data/1986/cbp/variables....</td>\n",
       "      <td>http://api.census.gov/data/1986/cbp/groups.html</td>\n",
       "      <td>http://api.census.gov/data/1986/cbp/sorts.html</td>\n",
       "      <td>http://api.census.gov/data/1986/cbp/examples.html</td>\n",
       "      <td>http://www.census.gov/developer/</td>\n",
       "      <td>http://api.census.gov/data/1986/cbp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987 County Business Patterns: Business Patterns</td>\n",
       "      <td>County Business Patterns (CBP) is an annual se...</td>\n",
       "      <td>1987</td>\n",
       "      <td>cbp</td>\n",
       "      <td>Aggregate</td>\n",
       "      <td>http://api.census.gov/data/1987/cbp/geography....</td>\n",
       "      <td>http://api.census.gov/data/1987/cbp/variables....</td>\n",
       "      <td>http://api.census.gov/data/1987/cbp/groups.html</td>\n",
       "      <td>http://api.census.gov/data/1987/cbp/sorts.html</td>\n",
       "      <td>http://api.census.gov/data/1987/cbp/examples.html</td>\n",
       "      <td>http://www.census.gov/developer/</td>\n",
       "      <td>http://api.census.gov/data/1987/cbp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988 County Business Patterns: Business Patterns</td>\n",
       "      <td>County Business Patterns (CBP) is an annual se...</td>\n",
       "      <td>1988</td>\n",
       "      <td>cbp</td>\n",
       "      <td>Aggregate</td>\n",
       "      <td>http://api.census.gov/data/1988/cbp/geography....</td>\n",
       "      <td>http://api.census.gov/data/1988/cbp/variables....</td>\n",
       "      <td>http://api.census.gov/data/1988/cbp/groups.html</td>\n",
       "      <td>http://api.census.gov/data/1988/cbp/sorts.html</td>\n",
       "      <td>http://api.census.gov/data/1988/cbp/examples.html</td>\n",
       "      <td>http://www.census.gov/developer/</td>\n",
       "      <td>http://api.census.gov/data/1988/cbp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1989 County Business Patterns: Business Patterns</td>\n",
       "      <td>County Business Patterns (CBP) is an annual se...</td>\n",
       "      <td>1989</td>\n",
       "      <td>cbp</td>\n",
       "      <td>Aggregate</td>\n",
       "      <td>http://api.census.gov/data/1989/cbp/geography....</td>\n",
       "      <td>http://api.census.gov/data/1989/cbp/variables....</td>\n",
       "      <td>http://api.census.gov/data/1989/cbp/groups.html</td>\n",
       "      <td>http://api.census.gov/data/1989/cbp/sorts.html</td>\n",
       "      <td>http://api.census.gov/data/1989/cbp/examples.html</td>\n",
       "      <td>http://www.census.gov/developer/</td>\n",
       "      <td>http://api.census.gov/data/1989/cbp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title  \\\n",
       "0                                     1648 datasets   \n",
       "1  1986 County Business Patterns: Business Patterns   \n",
       "2  1987 County Business Patterns: Business Patterns   \n",
       "3  1988 County Business Patterns: Business Patterns   \n",
       "4  1989 County Business Patterns: Business Patterns   \n",
       "\n",
       "                                         Description Vintage Dataset Name  \\\n",
       "0                                                NaN     NaN          NaN   \n",
       "1  County Business Patterns (CBP) is an annual se...    1986          cbp   \n",
       "2  County Business Patterns (CBP) is an annual se...    1987          cbp   \n",
       "3  County Business Patterns (CBP) is an annual se...    1988          cbp   \n",
       "4  County Business Patterns (CBP) is an annual se...    1989          cbp   \n",
       "\n",
       "  Dataset Type                                     Geography List  \\\n",
       "0          NaN                                                NaN   \n",
       "1    Aggregate  http://api.census.gov/data/1986/cbp/geography....   \n",
       "2    Aggregate  http://api.census.gov/data/1987/cbp/geography....   \n",
       "3    Aggregate  http://api.census.gov/data/1988/cbp/geography....   \n",
       "4    Aggregate  http://api.census.gov/data/1989/cbp/geography....   \n",
       "\n",
       "                                       Variable List  \\\n",
       "0                                                NaN   \n",
       "1  http://api.census.gov/data/1986/cbp/variables....   \n",
       "2  http://api.census.gov/data/1987/cbp/variables....   \n",
       "3  http://api.census.gov/data/1988/cbp/variables....   \n",
       "4  http://api.census.gov/data/1989/cbp/variables....   \n",
       "\n",
       "                                        Group List  \\\n",
       "0                                              NaN   \n",
       "1  http://api.census.gov/data/1986/cbp/groups.html   \n",
       "2  http://api.census.gov/data/1987/cbp/groups.html   \n",
       "3  http://api.census.gov/data/1988/cbp/groups.html   \n",
       "4  http://api.census.gov/data/1989/cbp/groups.html   \n",
       "\n",
       "                                         SortList  \\\n",
       "0                                             NaN   \n",
       "1  http://api.census.gov/data/1986/cbp/sorts.html   \n",
       "2  http://api.census.gov/data/1987/cbp/sorts.html   \n",
       "3  http://api.census.gov/data/1988/cbp/sorts.html   \n",
       "4  http://api.census.gov/data/1989/cbp/sorts.html   \n",
       "\n",
       "                                            Examples  \\\n",
       "0                                                NaN   \n",
       "1  http://api.census.gov/data/1986/cbp/examples.html   \n",
       "2  http://api.census.gov/data/1987/cbp/examples.html   \n",
       "3  http://api.census.gov/data/1988/cbp/examples.html   \n",
       "4  http://api.census.gov/data/1989/cbp/examples.html   \n",
       "\n",
       "            Developer Documentation                         API Base URL  \n",
       "0                               NaN                                  NaN  \n",
       "1  http://www.census.gov/developer/  http://api.census.gov/data/1986/cbp  \n",
       "2  http://www.census.gov/developer/  http://api.census.gov/data/1987/cbp  \n",
       "3  http://www.census.gov/developer/  http://api.census.gov/data/1988/cbp  \n",
       "4  http://www.census.gov/developer/  http://api.census.gov/data/1989/cbp  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your HTML file\n",
    "html_file_path = './data/CensusDataAPI_data.html'  # Update this path as needed\n",
    "\n",
    "# Define the columns that contain URLs\n",
    "url_columns = [\n",
    "    'Geography List',\n",
    "    'Variable List',\n",
    "    'Group List',\n",
    "    'SortList',\n",
    "    'Examples',\n",
    "    'Developer Documentation',\n",
    "    'API Base URL'\n",
    "]\n",
    "\n",
    "# Load and parse the HTML file\n",
    "with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "    soup = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "# Find all tables in the HTML\n",
    "tables = soup.find_all('table')\n",
    "print(f\"Number of tables found: {len(tables)}\")\n",
    "\n",
    "# Check if at least one table is found\n",
    "if not tables:\n",
    "    raise ValueError(\"No tables found in the HTML file.\")\n",
    "\n",
    "# Assuming your data is in the first table; adjust the index if necessary\n",
    "table = tables[0]\n",
    "\n",
    "# Extract table headers\n",
    "headers = [th.get_text(strip=True) for th in table.find_all('th')]\n",
    "print(f\"Headers: {headers}\")\n",
    "\n",
    "# Initialize a list to store each row's data\n",
    "rows = []\n",
    "\n",
    "# Iterate over each row in the table (skip header row)\n",
    "for tr in table.find_all('tr')[1:]:\n",
    "    cells = tr.find_all(['td', 'th'])\n",
    "    row_data = {}\n",
    "    for idx, cell in enumerate(cells):\n",
    "        # Get the header for the current cell\n",
    "        header = headers[idx] if idx < len(headers) else f'Column_{idx+1}'\n",
    "\n",
    "        if header in url_columns:\n",
    "            # Extract all href attributes from <a> tags\n",
    "            links = cell.find_all('a')\n",
    "            urls = [link.get('href') for link in links if link.get('href')]\n",
    "\n",
    "            # If no <a> tags, check if the cell contains a plain URL\n",
    "            if not urls:\n",
    "                cell_text = cell.get_text(strip=True)\n",
    "                if cell_text.startswith('http://') or cell_text.startswith('https://'):\n",
    "                    urls = [cell_text]\n",
    "\n",
    "            # Join multiple URLs with '; ' or set as None if no URLs found\n",
    "            row_data[header] = '; '.join(urls) if urls else None\n",
    "        else:\n",
    "            # For other columns, store the text\n",
    "            row_data[header] = cell.get_text(strip=True)\n",
    "    rows.append(row_data)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df_census = pd.DataFrame(rows)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_census.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee50096-e7b0-489e-ad4e-d03611c762ec",
   "metadata": {},
   "source": [
    "### 2.2 Parsing data set name column\n",
    "- There is hierarchial information embedded in this column, potenially use for the graph database createion\n",
    "- However, just in case, we're going to extract that, and grab the 'month' as time data we may want to use as a relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0f9a9675-5256-4612-bb90-eb8196400b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survey</th>\n",
       "      <th>Subtype1</th>\n",
       "      <th>Subtype2</th>\n",
       "      <th>Subtype3</th>\n",
       "      <th>Vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cbp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cbp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cbp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cbp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cps</td>\n",
       "      <td>basic</td>\n",
       "      <td>apr</td>\n",
       "      <td>None</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cps</td>\n",
       "      <td>basic</td>\n",
       "      <td>aug</td>\n",
       "      <td>None</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Survey Subtype1 Subtype2 Subtype3 Vintage\n",
       "0    NaN      NaN      NaN      NaN     NaN\n",
       "1    cbp     None     None     None    1986\n",
       "2    cbp     None     None     None    1987\n",
       "3    cbp     None     None     None    1988\n",
       "4    cbp     None     None     None    1989\n",
       "5    cps    basic      apr     None    1989\n",
       "6    cps    basic      aug     None    1989"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the delimiter used in the \"Dataset Name\" column\n",
    "delimiter = '›'\n",
    "\n",
    "# Split the \"Dataset Name\" into hierarchical levels, maximum of 3 splits (4 parts)\n",
    "hierarchy_split = df_census['Dataset Name'].str.split(delimiter, n=3, expand=True)\n",
    "\n",
    "# Rename the new columns based on hierarchy levels\n",
    "hierarchy_split = hierarchy_split.rename(columns={\n",
    "    0: 'Survey',\n",
    "    1: 'Subtype1',\n",
    "    2: 'Subtype2',\n",
    "    3: 'Subtype3'\n",
    "})\n",
    "\n",
    "# Concatenate the new hierarchy columns with the original DataFrame\n",
    "df_census = pd.concat([df_census, hierarchy_split], axis=1)\n",
    "\n",
    "# Display the first few rows after parsing\n",
    "df_census[['Survey', 'Subtype1', 'Subtype2', 'Subtype3', 'Vintage']].head(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517eb64b-576f-4c3f-91b3-bacc7c1c4254",
   "metadata": {},
   "source": [
    "### 2.3 Extract Month from subtype columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2d717132-2b9b-4515-b489-e1a243b1274d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survey</th>\n",
       "      <th>Subtype1</th>\n",
       "      <th>Subtype2</th>\n",
       "      <th>Subtype3</th>\n",
       "      <th>Month</th>\n",
       "      <th>Vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cbp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cbp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cbp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cbp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Survey Subtype1 Subtype2 Subtype3 Month Vintage\n",
       "0    NaN      NaN      NaN      NaN  None     NaN\n",
       "1    cbp     None     None     None  None    1986\n",
       "2    cbp     None     None     None  None    1987\n",
       "3    cbp     None     None     None  None    1988\n",
       "4    cbp     None     None     None  None    1989"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a list of month abbreviations for identification (lowercase for matching)\n",
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun',\n",
    "          'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "# Function to identify and extract month from subtype columns\n",
    "def extract_month(row):\n",
    "    for subtype_col in ['Subtype1', 'Subtype2', 'Subtype3']:\n",
    "        if pd.notnull(row[subtype_col]):\n",
    "            if row[subtype_col].strip().lower() in months:\n",
    "                return row[subtype_col].strip().capitalize()\n",
    "    return None\n",
    "\n",
    "# Apply the function to create a new 'Month' column\n",
    "df_census['Month'] = df_census.apply(extract_month, axis=1)\n",
    "\n",
    "# Remove the month from the subtype columns to avoid duplication\n",
    "for subtype_col in ['Subtype1', 'Subtype2', 'Subtype3']:\n",
    "    df_census[subtype_col] = df_census[subtype_col].apply(\n",
    "        lambda x: None if pd.notnull(x) and x.strip().lower() in months else x\n",
    "    )\n",
    "\n",
    "# Display the first few rows after extracting 'Month'\n",
    "df_census[['Survey', 'Subtype1', 'Subtype2', 'Subtype3', 'Month', 'Vintage']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3a4c6-d68e-4eb5-a44d-ae7d49e114a4",
   "metadata": {},
   "source": [
    "### 2.4 Get a unique ID for each row\n",
    "1. Use the identifier from the API URL which is a JSON file that contains a unique identifier (aka KEY) for each dataset.\n",
    "2. Why Use identifier as the Key:\n",
    "- Uniqueness: The identifier provides a unique reference for each dataset, ensuring there are no duplicates.\n",
    "- Consistency: Using a standardized key helps in linking data across different sources and maintaining data integrity within your knowledge graph.\n",
    "- Efficiency: It simplifies data retrieval and relationships within the knowledge graph.\n",
    "\n",
    "#### 2.4.1 First, drop the first row contains the number of records and a bunch of na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "330aeb88-3d11-48ff-a491-005304762ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping row:\n",
      "           Title Description Vintage Dataset Name Dataset Type Geography List  \\\n",
      "0  1648 datasets         NaN     NaN          NaN          NaN            NaN   \n",
      "\n",
      "  Variable List Group List SortList Examples Developer Documentation  \\\n",
      "0           NaN        NaN      NaN      NaN                     NaN   \n",
      "\n",
      "  API Base URL Survey Subtype1 Subtype2 Subtype3 Month identifier id_name  \n",
      "0          NaN    NaN      NaN      NaN      NaN  None       None    None  \n",
      "\n",
      "After dropping row:\n",
      "                                              Title  \\\n",
      "0  1986 County Business Patterns: Business Patterns   \n",
      "\n",
      "                                         Description Vintage Dataset Name  \\\n",
      "0  County Business Patterns (CBP) is an annual se...    1986          cbp   \n",
      "\n",
      "  Dataset Type                                     Geography List  \\\n",
      "0    Aggregate  http://api.census.gov/data/1986/cbp/geography....   \n",
      "\n",
      "                                       Variable List  \\\n",
      "0  http://api.census.gov/data/1986/cbp/variables....   \n",
      "\n",
      "                                        Group List  \\\n",
      "0  http://api.census.gov/data/1986/cbp/groups.html   \n",
      "\n",
      "                                         SortList  \\\n",
      "0  http://api.census.gov/data/1986/cbp/sorts.html   \n",
      "\n",
      "                                            Examples  \\\n",
      "0  http://api.census.gov/data/1986/cbp/examples.html   \n",
      "\n",
      "            Developer Documentation                         API Base URL  \\\n",
      "0  http://www.census.gov/developer/  http://api.census.gov/data/1986/cbp   \n",
      "\n",
      "  Survey Subtype1 Subtype2 Subtype3 Month  \\\n",
      "0    cbp     None     None     None  None   \n",
      "\n",
      "                              identifier  id_name  \n",
      "0  http://api.census.gov/data/id/CBP1986  CBP1986  \n"
     ]
    }
   ],
   "source": [
    "# Removeing first row where all elements are NaN\n",
    "# Before removing first row\n",
    "print(\"Before dropping row:\")\n",
    "print(df_census.head(1))\n",
    "\n",
    "# Removing first row\n",
    "df_census = df_census.drop(0).reset_index(drop=True)\n",
    "\n",
    "# After removing first row\n",
    "print(\"\\nAfter dropping row:\")\n",
    "print(df_census.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee787fc-9818-48b4-95d1-734e503b32d0",
   "metadata": {},
   "source": [
    "#### 2.4.2 Extracting the Identifier Field to create key column SurveyID\n",
    "- Robust error handling was added do account for network issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8a2e6706-6bbb-4cae-b8b8-77c54b88e52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current DataFrame Columns:\n",
      "['Title', 'Description', 'Vintage', 'Dataset Name', 'Dataset Type', 'Geography List', 'Variable List', 'Group List', 'SortList', 'Examples', 'Developer Documentation', 'API Base URL', 'Survey', 'Subtype1', 'Subtype2', 'Subtype3', 'Month', 'identifier', 'id_name']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching identifiers: 100%|█████████████████| 1648/1648 [02:27<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After extracting 'identifier' and 'SurveyID':\n",
      "  Survey Subtype1 Subtype2 Subtype3 Vintage  \\\n",
      "0    cbp     None     None     None    1986   \n",
      "1    cbp     None     None     None    1987   \n",
      "2    cbp     None     None     None    1988   \n",
      "3    cbp     None     None     None    1989   \n",
      "4    cps    basic     None     None    1989   \n",
      "\n",
      "                                      identifier        SurveyID  \n",
      "0          http://api.census.gov/data/id/CBP1986         CBP1986  \n",
      "1          http://api.census.gov/data/id/CBP1987         CBP1987  \n",
      "2          http://api.census.gov/data/id/CBP1988         CBP1988  \n",
      "3          http://api.census.gov/data/id/CBP1989         CBP1989  \n",
      "4  https://api.census.gov/data/id/CPSBASIC198904  CPSBASIC198904  \n",
      "\n",
      "Number of missing identifiers: 0\n",
      "All identifiers were successfully extracted.\n",
      "\n",
      "Updated DataFrame with 'identifier' and 'SurveyID' successfully saved to ./data/df_census_with_identifiers.csv\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Remove rows where all elements are NaN\n",
    "df_census.dropna(how='all', inplace=True)\n",
    "df_census.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display current DataFrame columns to confirm\n",
    "print(\"Current DataFrame Columns:\")\n",
    "print(df_census.columns.tolist())\n",
    "\n",
    "# Step 1: Set up a session with retries (move this outside the function)\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5,  # Total number of retries\n",
    "    backoff_factor=1,  # Time to wait between retries (exponential backoff)\n",
    "    status_forcelist=[500, 502, 503, 504],  # Retry on these HTTP status codes\n",
    "    allowed_methods=['GET']  # Use 'allowed_methods' instead of 'method_whitelist'\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# Step 2: Define a Function to Extract 'identifier' and 'SurveyID' from a Single URL with Retries\n",
    "\n",
    "def extract_identifier(url, session):\n",
    "    \"\"\"\n",
    "    Fetches JSON data from the given URL and extracts the 'identifier' and 'SurveyID'.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The API Base URL pointing to the JSON file.\n",
    "        session (requests.Session): The session object with retry strategy.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (identifier, SurveyID) where:\n",
    "               - identifier (str or None): The full identifier URL.\n",
    "               - SurveyID (str or None): The part of the identifier after '/id/'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the URL is a valid string\n",
    "        if not isinstance(url, str) or pd.isna(url):\n",
    "            print(\"Invalid URL encountered.\")\n",
    "            return None, None\n",
    "\n",
    "        # Fetch the JSON data from the URL\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        json_data = response.json()\n",
    "        \n",
    "        # Extract the 'identifier' field from the first dataset\n",
    "        dataset_list = json_data.get('dataset', [])\n",
    "        if isinstance(dataset_list, list) and len(dataset_list) > 0:\n",
    "            identifier = dataset_list[0].get('identifier', None)\n",
    "        else:\n",
    "            identifier = None\n",
    "        \n",
    "        # Extract the 'SurveyID' by splitting the 'identifier' at '/id/'\n",
    "        if identifier and '/id/' in identifier:\n",
    "            SurveyID = identifier.split('/id/')[-1]\n",
    "        else:\n",
    "            SurveyID = None\n",
    "        \n",
    "        return identifier, SurveyID\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout error occurred while fetching {url}\")\n",
    "        return None, None\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"Connection error occurred while fetching {url}\")\n",
    "        return None, None\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred while fetching {url}: {e}\")\n",
    "        return None, None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while fetching {url}: {e}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Step 3: Apply the Extraction Function to All Rows\n",
    "\n",
    "# Initialize lists to store the results\n",
    "identifiers = []\n",
    "survey_ids = []\n",
    "\n",
    "# Iterate over each API Base URL and extract identifiers\n",
    "for url in tqdm(df_census['API Base URL'], desc=\"Fetching identifiers\"):\n",
    "    identifier, SurveyID = extract_identifier(url, session)\n",
    "    identifiers.append(identifier)\n",
    "    survey_ids.append(SurveyID)\n",
    "\n",
    "# Step 4: Add the Extracted Data to the DataFrame\n",
    "\n",
    "df_census['identifier'] = identifiers\n",
    "df_census['SurveyID'] = survey_ids\n",
    "\n",
    "# Display the first few rows to verify the extraction\n",
    "print(\"\\nAfter extracting 'identifier' and 'SurveyID':\")\n",
    "print(df_census[['Survey', 'Subtype1', 'Subtype2', 'Subtype3', 'Vintage', 'identifier', 'SurveyID']].head())\n",
    "\n",
    "# Step 5: Handle Missing Identifiers\n",
    "\n",
    "# Check for missing identifiers\n",
    "missing_identifiers = df_census['identifier'].isnull().sum()\n",
    "print(f\"\\nNumber of missing identifiers: {missing_identifiers}\")\n",
    "\n",
    "# If there are missing identifiers, display them and save to a separate CSV for manual correction\n",
    "if missing_identifiers > 0:\n",
    "    print(\"\\nRows with missing identifiers:\")\n",
    "    missing_identifiers_df = df_census[df_census['identifier'].isnull()]\n",
    "    print(missing_identifiers_df[['Survey', 'API Base URL']].head())\n",
    "    \n",
    "    # Save these rows to a separate CSV file\n",
    "    missing_identifiers_df.to_csv('./data/missing_identifiers.csv', index=False)\n",
    "    print(\"\\nSaved rows with missing identifiers to 'missing_identifiers.csv' for manual correction.\")\n",
    "else:\n",
    "    print(\"All identifiers were successfully extracted.\")\n",
    "\n",
    "# Step 6: Save the Updated DataFrame for Backup\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "output_file = './data/df_census_with_identifiers.csv'\n",
    "df_census.to_csv(output_file, index=False)\n",
    "print(f\"\\nUpdated DataFrame with 'identifier' and 'SurveyID' successfully saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451ae55-8985-4c4e-84f7-f25c1d8c9086",
   "metadata": {},
   "source": [
    "#### 2.4.3 Manually fixing errors (Only if needed)\n",
    "- Recommended: Rerun! Maybe wait if there is a network issue.\n",
    "- Manualy fixing is an acceptable strategy, but prone to human error.\n",
    "- robust error handling was added to avoid errors, but stuff happens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69656f8f-acf7-4ab0-87f9-6edfbc3b12cf",
   "metadata": {},
   "source": [
    "## 3. Create External Tables and Linkages for Survey Table\n",
    "1. Several columns have URLs that link to other tables: 'Geography List', 'Variable List', 'Group List', 'SortList', 'Examples'\n",
    "2. In order to avoid issues with data scraping, collect a copy of the data with linkages back to SurveyID\n",
    "3. Variables and Group have more complex structures that will require additional processing to capture the information\n",
    "4. 'SortList' is believed to be a parameter and probably doesn't have any info, but we'll look anyways\n",
    "\n",
    "> We will programatically create the graph database from these tables. The CSV files are an intermediary step, but one done out of practical necessity to avoid retrieval issues and create a solid foundation to work from with all data local to the compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af541e7-11d3-41b3-812a-6a7a78024f79",
   "metadata": {},
   "source": [
    "## 3.1 Process 'Examples' Column\n",
    "- This contains example API calls to use this survey\n",
    "- For building the graph databases later, we will document if the relationship exists (Has Example) ...\n",
    "- If there is no relationship, we won't waste cycles trying to look or retrieve anything.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ff3fa5d-762f-476b-ba5e-98701af3f018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Examples: 100%|███████████████| 1648/1648 [04:20<00:00,  6.32items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction completed successfully!\n",
      "Total rows processed: 1648\n",
      "Total examples found: 22649\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import warnings\n",
    "import re\n",
    "from progress_tracker import track_progress\n",
    "from web_data_extraction import create_robust_session, fetch_url\n",
    "\n",
    "# Suppress FutureWarning for pd.read_html\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Passing literal html to 'read_html' is deprecated\")\n",
    "\n",
    "def is_summary_row(row):\n",
    "    \"\"\"\n",
    "    Check if a row is a summary row (contains count of items/examples).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    row : pandas.Series\n",
    "        A row from the table\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if the row is a summary row\n",
    "    \"\"\"\n",
    "    row_str = ' '.join(str(val).lower() for val in row.values)\n",
    "    summary_patterns = [\n",
    "        r'\\d+\\s*items?',\n",
    "        r'\\d+\\s*examples?',\n",
    "        r'\\d+\\s*groups?',\n",
    "        'n/a',\n",
    "        'total'\n",
    "    ]\n",
    "    return any(re.search(pattern, row_str.lower()) for pattern in summary_patterns)\n",
    "\n",
    "def clean_table(df):\n",
    "    \"\"\"\n",
    "    Clean a table by removing summary rows and empty rows.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The table to clean\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Cleaned table\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "        \n",
    "    # Remove summary rows\n",
    "    df = df[~df.apply(is_summary_row, axis=1)]\n",
    "    \n",
    "    # Remove rows where all values are NA or empty\n",
    "    df = df.dropna(how='all')\n",
    "    df = df[~df.apply(lambda x: x.astype(str).str.strip().eq('').all(), axis=1)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_example_url(session, url, survey_id):\n",
    "    \"\"\"\n",
    "    Process a single Examples URL to extract its data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check for invalid URL\n",
    "        if pd.isna(url) or not isinstance(url, str):\n",
    "            return \"No Example\", None\n",
    "            \n",
    "        # Fetch URL content\n",
    "        html_content = fetch_url(url, session)\n",
    "        if not html_content:\n",
    "            return \"No Example\", None\n",
    "            \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Try to find tables\n",
    "        tables = pd.read_html(StringIO(str(soup)))\n",
    "        \n",
    "        if not tables:\n",
    "            return \"No Example\", None\n",
    "            \n",
    "        # Process the first table\n",
    "        example_table = tables[0]\n",
    "        \n",
    "        # Clean column names\n",
    "        example_table.columns = example_table.columns.str.strip()\n",
    "        \n",
    "        # Clean the table\n",
    "        clean_df = clean_table(example_table)\n",
    "        \n",
    "        if clean_df.empty:\n",
    "            return \"No Example\", None\n",
    "            \n",
    "        # Convert to records\n",
    "        examples = []\n",
    "        for _, row in clean_df.iterrows():\n",
    "            example_dict = row.to_dict()\n",
    "            example_dict['SurveyID'] = survey_id\n",
    "            examples.append(example_dict)\n",
    "            \n",
    "        return \"Has Example\", examples\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {str(e)}\")\n",
    "        return \"No Example\", None\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load survey data\n",
    "        survey_df = pd.read_csv('../data/data_extraction/df_census_with_identifiers.csv')\n",
    "        \n",
    "        # Initialize session and examples data list\n",
    "        session = create_robust_session()\n",
    "        examples_data = []\n",
    "        \n",
    "        # Process each row\n",
    "        for index, row in track_progress(survey_df.iterrows(), description=\"Processing Examples\", total=survey_df.shape[0]):\n",
    "            try:\n",
    "                survey_id = row['id_name']\n",
    "                example_url = row['Examples']\n",
    "                \n",
    "                # Process the URL\n",
    "                has_example, example_list = process_example_url(session, example_url, survey_id)\n",
    "                \n",
    "                # Update survey DataFrame\n",
    "                survey_df.at[index, 'Has Example'] = has_example\n",
    "                \n",
    "                # Add valid examples to the list\n",
    "                if example_list:\n",
    "                    examples_data.extend(example_list)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Save updated survey data\n",
    "        survey_df.to_csv('../data/data_extraction/SurveyNode.csv', index=False)\n",
    "        \n",
    "        # Save examples data if we have any\n",
    "        if examples_data:\n",
    "            examples_df = pd.DataFrame(examples_data)\n",
    "            examples_df.to_csv('../data/data_extraction/ExamplesNode.csv', index=False)\n",
    "        \n",
    "        print(\"Data extraction completed successfully!\")\n",
    "        print(f\"Total rows processed: {len(survey_df)}\")\n",
    "        print(f\"Total examples found: {len(examples_data)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dee76a-65b0-4afa-b4d6-051d3446458f",
   "metadata": {},
   "source": [
    "## 3.1 Process 'SortList' Column\n",
    "- Same strategy as above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40783659-1dbe-4c8a-a91a-09ec962ed173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SortList: 100%|███████████████| 1648/1648 [09:42<00:00,  2.83items/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed successfully!\n",
      "Total rows processed: 1648\n",
      "Total sort nodes found: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from progress_tracker import track_progress\n",
    "from web_data_extraction import create_robust_session, fetch_url\n",
    "\n",
    "# Dynamically add the src directory to sys.path\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"../src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "def load_dataframe(file_path):\n",
    "    \"\"\"Load a CSV file into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def save_dataframe(df, file_path):\n",
    "    \"\"\"Save a pandas DataFrame to a CSV file.\"\"\"\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def is_empty_table(table):\n",
    "    \"\"\"\n",
    "    Check if a table is effectively empty (contains only N/A or no meaningful data).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    table : pandas.DataFrame\n",
    "        The table to check\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if the table is empty or contains only N/A values\n",
    "    \"\"\"\n",
    "    if table.empty:\n",
    "        return True\n",
    "        \n",
    "    # Check if all values are N/A or empty\n",
    "    is_na = table.apply(lambda x: x.astype(str).str.strip().isin(['N/A', '', 'nan']).all())\n",
    "    if is_na.all():\n",
    "        return True\n",
    "        \n",
    "    # Check for \"0 groups\" marker\n",
    "    has_zero_groups = table.apply(lambda x: x.astype(str).str.contains('0 groups', case=False)).any().any()\n",
    "    return has_zero_groups\n",
    "\n",
    "def process_sort_list(row):\n",
    "    \"\"\"\n",
    "    Process the SortList column to determine if it has meaningful data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    row : pandas.Series\n",
    "        A single row of the DataFrame\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (\"Has Sort\"/\"No Sort\", list of nodes or None)\n",
    "    \"\"\"\n",
    "    sort_list_url = row.get(\"SortList\")\n",
    "    survey_id = row.get(\"SurveyID\")\n",
    "    \n",
    "    # Early return for invalid URLs\n",
    "    if pd.isna(sort_list_url) or not isinstance(sort_list_url, str):\n",
    "        return \"No Sort\", None\n",
    "    \n",
    "    try:\n",
    "        # Fetch URL content\n",
    "        session = create_robust_session()\n",
    "        html_content = fetch_url(sort_list_url, session)\n",
    "        \n",
    "        if not html_content:\n",
    "            return \"No Sort\", None\n",
    "            \n",
    "        # Parse tables\n",
    "        tables = pd.read_html(html_content)\n",
    "        \n",
    "        if not tables:\n",
    "            return \"No Sort\", None\n",
    "            \n",
    "        sort_table = tables[0]\n",
    "        \n",
    "        # Check if table is effectively empty\n",
    "        if is_empty_table(sort_table):\n",
    "            return \"No Sort\", None\n",
    "        \n",
    "        # Clean column names\n",
    "        sort_table.columns = sort_table.columns.str.strip()\n",
    "        \n",
    "        # Verify required columns exist\n",
    "        required_columns = [\"SortItem\", \"Description\"]\n",
    "        if not all(col in sort_table.columns for col in required_columns):\n",
    "            print(f\"Warning: Missing required columns for SurveyID: {survey_id}\")\n",
    "            return \"No Sort\", None\n",
    "        \n",
    "        # Remove metadata rows and empty entries\n",
    "        sort_table = sort_table[~sort_table.apply(lambda x: x.astype(str).str.contains('groups|N/A', case=False).any(), axis=1)]\n",
    "        sort_table = sort_table.dropna(how='all')\n",
    "        \n",
    "        # If no valid rows remain after cleaning\n",
    "        if sort_table.empty:\n",
    "            return \"No Sort\", None\n",
    "        \n",
    "        # Create nodes from valid rows\n",
    "        sort_nodes = []\n",
    "        for _, valid_row in sort_table.iterrows():\n",
    "            if pd.notna(valid_row[\"SortItem\"]) and str(valid_row[\"SortItem\"]).strip().upper() != \"N/A\":\n",
    "                sort_nodes.append({\n",
    "                    \"SortItem\": valid_row[\"SortItem\"],\n",
    "                    \"Description\": valid_row.get(\"Description\", \"N/A\"),\n",
    "                    \"SurveyID\": survey_id\n",
    "                })\n",
    "        \n",
    "        return \"Has Sort\", sort_nodes if sort_nodes else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing SurveyID {survey_id}: {str(e)}\")\n",
    "        return \"No Sort\", None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Load SurveyNode data\n",
    "        survey_df = load_dataframe('../data/data_extraction/SurveyNode.csv')\n",
    "        \n",
    "        # Initialize empty list for sort nodes\n",
    "        all_sort_nodes = []\n",
    "        \n",
    "        # Process each row\n",
    "        for index, row in track_progress(survey_df.iterrows(), description=\"Processing SortList\", total=survey_df.shape[0]):\n",
    "            has_sort, nodes = process_sort_list(row)\n",
    "            survey_df.at[index, \"Has Sort\"] = has_sort\n",
    "            \n",
    "            if nodes:\n",
    "                all_sort_nodes.extend(nodes)\n",
    "        \n",
    "        # Save updated SurveyNode data\n",
    "        save_dataframe(survey_df, '../data/data_extraction/SurveyNode.csv')\n",
    "        \n",
    "        # Save SortListNode data if we have any nodes\n",
    "        if all_sort_nodes:\n",
    "            sort_list_df = pd.DataFrame(all_sort_nodes)\n",
    "            save_dataframe(sort_list_df, '../data/data_extraction/SortListNode.csv')\n",
    "        \n",
    "        print(f\"Processing completed successfully!\")\n",
    "        print(f\"Total rows processed: {len(survey_df)}\")\n",
    "        print(f\"Total sort nodes found: {len(all_sort_nodes)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e1130f-0799-4579-9d35-05a409be3ab7",
   "metadata": {},
   "source": [
    "## 3.3 Process 'Geography List' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f388633-5eed-40a4-92e8-4284585f0e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Has Geography: 100%|██████████| 1648/1648 [08:31<00:00,  3.22items/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geography List processing completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from progress_tracker import track_progress\n",
    "from web_data_extraction import create_robust_session, fetch_url\n",
    "\n",
    "\n",
    "def safe_strip(value):\n",
    "    \"\"\"\n",
    "    Safely strip whitespace from a value, handling different data types.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    value : any\n",
    "        The value to strip whitespace from.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Stripped string value, or empty string if input was None/NaN.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    return str(value).strip()\n",
    "\n",
    "\n",
    "def load_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "def save_dataframe(df, file_path):\n",
    "    \"\"\"\n",
    "    Save a pandas DataFrame to a CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "def clean_geography_hierarchy(hierarchy_text):\n",
    "    \"\"\"\n",
    "    Clean the geography hierarchy text by removing unnecessary markers and whitespace.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hierarchy_text : str\n",
    "        The geography hierarchy text to clean.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Cleaned hierarchy text.\n",
    "    \"\"\"\n",
    "    if pd.isna(hierarchy_text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string in case we received a non-string type\n",
    "    hierarchy_text = str(hierarchy_text)\n",
    "    \n",
    "    # Remove {.hier} and {.hier-sep} markers\n",
    "    cleaned = hierarchy_text.replace(\"{.hier}\", \"\").replace(\"{.hier-sep}\", \"\")\n",
    "    \n",
    "    # Remove square brackets\n",
    "    cleaned = cleaned.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    \n",
    "    # Clean up any extra whitespace\n",
    "    cleaned = \" \".join(cleaned.split())\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def process_geography_list(row):\n",
    "    \"\"\"\n",
    "    Process the Geography List column to determine if it has meaningful data and prepare a node structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    row : pandas.Series\n",
    "        A single row of the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple:\n",
    "        - str: \"Has Geography\" or \"No Geography\" based on the presence of meaningful data.\n",
    "        - list or None: Extracted Geography List data as a list of nodes, or None if no meaningful data exists.\n",
    "    \"\"\"\n",
    "    # Use Geography List URL and SurveyID as key for linking nodes\n",
    "    geography_list_url = row.get(\"Geography List\")\n",
    "    survey_id = row.get(\"SurveyID\")\n",
    "\n",
    "    # If the Geography List URL is missing or invalid, mark as \"No Geography\"\n",
    "    if pd.isna(geography_list_url) or not isinstance(geography_list_url, str):\n",
    "        return \"No Geography\", None\n",
    "\n",
    "    # Fetch the URL content\n",
    "    session = create_robust_session()\n",
    "    html_content = fetch_url(geography_list_url, session)\n",
    "\n",
    "    # If no content is retrieved, mark as \"No Geography\"\n",
    "    if not html_content:\n",
    "        return \"No Geography\", None\n",
    "\n",
    "    # Parse the HTML to extract tables\n",
    "    try:\n",
    "        tables = pd.read_html(html_content)\n",
    "    except ValueError:\n",
    "        # No tables found in the HTML\n",
    "        return \"No Geography\", None\n",
    "\n",
    "    if not tables or len(tables) == 0:\n",
    "        return \"No Geography\", None\n",
    "\n",
    "    geography_table = tables[0]\n",
    "\n",
    "    # Remove rows with 'items' in any column (e.g., \"1 item\" footer rows)\n",
    "    geography_table = geography_table[~geography_table.apply(lambda x: x.astype(str).str.contains('item', case=False).any(), axis=1)]\n",
    "\n",
    "    # Remove rows where \"Geography Hierarchy\" contains \"(default geography)\"\n",
    "    geography_table = geography_table[\n",
    "        ~geography_table[\"Geography Hierarchy\"].astype(str).str.contains(r\"\\(default geography\\)\", na=False, case=False)\n",
    "    ]\n",
    "\n",
    "    # Remove rows where key columns (\"Geography Level\" or \"Geography Hierarchy\") are all `N/A`\n",
    "    geography_table = geography_table.dropna(how=\"all\", subset=[\"Geography Level\", \"Geography Hierarchy\"])\n",
    "\n",
    "    # If no meaningful rows are left, mark as \"No Geography\"\n",
    "    if geography_table.empty:\n",
    "        return \"No Geography\", None\n",
    "\n",
    "    # Clean column names - remove any leading/trailing whitespace\n",
    "    geography_table.columns = geography_table.columns.str.strip()\n",
    "\n",
    "    # Convert remaining rows into GeographyListNode structure\n",
    "    geography_nodes = []\n",
    "    for _, valid_row in geography_table.iterrows():\n",
    "        # Clean the hierarchy text\n",
    "        hierarchy = clean_geography_hierarchy(valid_row.get(\"Geography Hierarchy\", \"\"))\n",
    "        \n",
    "        geography_nodes.append({\n",
    "            \"ReferenceDate\": safe_strip(valid_row.get(\"Reference Date\", \"\")),\n",
    "            \"GeographyLevel\": safe_strip(valid_row.get(\"Geography Level\", \"\")),\n",
    "            \"GeographyHierarchy\": hierarchy,\n",
    "            \"Limit\": safe_strip(valid_row.get(\"Limit\", \"\")),\n",
    "            \"SurveyID\": safe_strip(survey_id)\n",
    "        })\n",
    "\n",
    "    return \"Has Geography\", geography_nodes\n",
    "\n",
    "\n",
    "def process_rows(df, column_name, processing_function):\n",
    "    \"\"\"\n",
    "    Process rows in a DataFrame for a specific column using a processing function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame to process.\n",
    "    column_name : str\n",
    "        The column to process.\n",
    "    processing_function : function\n",
    "        The function to apply to each row for processing.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple:\n",
    "        - pandas.DataFrame: Updated DataFrame with processing results in the specified column.\n",
    "        - list: List of nodes created during processing.\n",
    "    \"\"\"\n",
    "    nodes = []\n",
    "\n",
    "    # Add the column if it doesn't exist\n",
    "    if column_name not in df.columns:\n",
    "        df[column_name] = None\n",
    "\n",
    "    # Process each row\n",
    "    for index, row in track_progress(df.iterrows(), description=f\"Processing {column_name}\", total=df.shape[0]):\n",
    "        has_data, node_list = processing_function(row)\n",
    "        df.at[index, column_name] = has_data\n",
    "\n",
    "        if node_list:\n",
    "            nodes.extend(node_list)\n",
    "\n",
    "    return df, nodes\n",
    "\n",
    "\n",
    "# Main Script Logic\n",
    "\n",
    "# Load the SurveyNode DataFrame\n",
    "survey_df = load_dataframe('../data/data_extraction/SurveyNode.csv')\n",
    "\n",
    "# Process Geography List column and update Has Geography status\n",
    "survey_df, geography_nodes = process_rows(survey_df, \"Has Geography\", process_geography_list)\n",
    "\n",
    "# Save the updated SurveyNode DataFrame\n",
    "save_dataframe(survey_df, '../data/data_extraction/SurveyNode.csv')\n",
    "\n",
    "# Save the GeographyListNode if nodes exist\n",
    "if geography_nodes:\n",
    "    geography_list_df = pd.DataFrame(geography_nodes)\n",
    "    save_dataframe(geography_list_df, '../data/data_extraction/GeographyListNode.csv')\n",
    "\n",
    "print(\"Geography List processing completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62616405-90b9-47a8-ac9d-2885451c32cb",
   "metadata": {},
   "source": [
    "## 3.4 Group Lists\n",
    "- Check to see if there are groups (collections of selected variables) tied to this survey\n",
    "- If there are none, then 'no groups' will be our node relationship\n",
    "- If \"Has Groups\" we need to create the table of survey groups\n",
    "- Each 'group' is specific to the individual survey. Sometimes variables change year to year for the same survey, hence the need to keep everything 'unique' for now anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3f3410-c821-4bb5-97bf-425dca938fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Group List: 100%|█████████████| 1648/1648 [14:17<00:00,  1.92items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey Group processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from web_data_extraction import create_robust_session, fetch_url\n",
    "from progress_tracker import track_progress\n",
    "import json\n",
    "\n",
    "def load_json_from_url(url, session):\n",
    "    \"\"\"Load JSON data from a URL using the provided session.\"\"\"\n",
    "    response = session.get(url)\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            return response.json()\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def process_group_json(row):\n",
    "    \"\"\"Process a single row to fetch and process the group JSON data.\"\"\"\n",
    "    group_list_url = row.get(\"Group List\")\n",
    "    survey_id = row.get(\"SurveyID\")\n",
    "\n",
    "    # Handle missing or invalid URL\n",
    "    if pd.isna(group_list_url) or not isinstance(group_list_url, str):\n",
    "        return \"No Group\", None\n",
    "\n",
    "    # Modify URL to fetch JSON\n",
    "    group_list_url = group_list_url.replace(\".html\", \".json\")\n",
    "\n",
    "    # Create session and fetch JSON\n",
    "    session = create_robust_session()\n",
    "    json_data = load_json_from_url(group_list_url, session)\n",
    "\n",
    "    # If JSON data is empty or no groups, mark \"No Group\"\n",
    "    if not json_data or not json_data.get(\"groups\"):\n",
    "        return \"No Group\", None\n",
    "\n",
    "    # Parse groups from JSON data\n",
    "    group_list = []\n",
    "    for group in json_data.get(\"groups\", []):\n",
    "        group_list.append({\n",
    "            \"SurveyID\": survey_id,\n",
    "            \"GroupName\": group.get(\"name\", \"\"),\n",
    "            \"GroupDescription\": group.get(\"description\", \"\"),\n",
    "            \"GroupLink\": group.get(\"variables\", \"\")\n",
    "        })\n",
    "\n",
    "    return \"Has Group\", group_list\n",
    "\n",
    "def process_survey_groups(df, column_name):\n",
    "    \"\"\"\n",
    "    Process the Group List column to determine if it has meaningful data\n",
    "    and create SurveyGroupNode data.\n",
    "    \"\"\"\n",
    "    survey_group_nodes = []\n",
    "\n",
    "    # Add the column if it doesn't exist\n",
    "    if column_name not in df.columns:\n",
    "        df[column_name] = None\n",
    "\n",
    "    # Add SurveyGroupID column\n",
    "    if \"SurveyGroupID\" not in df.columns:\n",
    "        df[\"SurveyGroupID\"] = \"NA\"\n",
    "\n",
    "    # Process each row\n",
    "    for index, row in track_progress(df.iterrows(), description=\"Processing Group List\", total=df.shape[0]):\n",
    "        has_group, group_list = process_group_json(row)\n",
    "        df.at[index, column_name] = has_group\n",
    "\n",
    "        if group_list:\n",
    "            # Generate SurveyGroupID for each group\n",
    "            survey_group_ids = []\n",
    "            for group in group_list:\n",
    "                survey_group_id = f\"{row['SurveyID']}_{group['GroupName']}\"\n",
    "                group[\"SurveyGroupID\"] = survey_group_id\n",
    "                survey_group_ids.append(survey_group_id)\n",
    "\n",
    "            # Update SurveyGroupID column in SurveyNode table\n",
    "            df.at[index, \"SurveyGroupID\"] = \";\".join(survey_group_ids)  # Join multiple IDs with a semicolon\n",
    "\n",
    "            # Append group nodes to the list\n",
    "            survey_group_nodes.extend(group_list)\n",
    "\n",
    "    return df, survey_group_nodes\n",
    "\n",
    "# Main Script\n",
    "\n",
    "# Load the SurveyNode DataFrame\n",
    "survey_df = pd.read_csv(\"../data/data_extraction/SurveyNode-ex-srt-geo.csv\")\n",
    "\n",
    "# Process the Group List and generate SurveyGroupNode table\n",
    "survey_df, survey_group_nodes = process_survey_groups(survey_df, \"Has Group\")\n",
    "\n",
    "# Save the updated SurveyNode table\n",
    "survey_df.to_csv(\"../data/data_extraction/SurveyNode-ex-srt-geo-grp.csv\", index=False)\n",
    "\n",
    "# Save the SurveyGroupNode table\n",
    "if survey_group_nodes:\n",
    "    survey_group_df = pd.DataFrame(survey_group_nodes)\n",
    "    survey_group_df.to_csv(\"../data/data_extraction/SurveyGroupNode.csv\", index=False)\n",
    "\n",
    "print(\"Survey Group processing completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b2bb6-23c7-45eb-9d5c-2c8eba157b5b",
   "metadata": {},
   "source": [
    "## 3.5 Build the GroupNode table with all groups\n",
    "- This is more complex due to the fact that the HTML table for a group (group.html) contains more information provided by the group.json PLUS each variable's individual json file (name column is link to the variable.json)\n",
    "- We need to break this up into two parts:\n",
    "  1. extract the group.html data\n",
    "  2. enrich with additional variable metadata (critical: 'attribute' and 'attribute of' are relational data for our graph database!)\n",
    "\n",
    "### 3.5.1 Extract the Group.html info\n",
    "- We will break this up by first getting all the html table info\n",
    "- A batch size of 500 with the ability to restart at the last batch (at 64k rows, it's long)\n",
    "- An error log, which we can try to ingest from\n",
    "- adding extra columns, linkage info (eg SurveyID and SurveyGroupID)storing both the variable name and the variable.json link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fe411f6-3eb7-4848-b59b-bd7d97f99b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 already processed. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:54<00:00,  9.25items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_001.csv\n",
      "Chunk 1 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:58<00:00,  8.60items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_002.csv\n",
      "Chunk 2 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:57<00:00,  8.67items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_003.csv\n",
      "Chunk 3 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:56<00:00,  8.79items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_004.csv\n",
      "Chunk 4 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:56<00:00,  8.78items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_005.csv\n",
      "Chunk 5 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:57<00:00,  8.68items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_006.csv\n",
      "Chunk 6 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:57<00:00,  8.69items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_007.csv\n",
      "Chunk 7 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:57<00:00,  8.73items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_008.csv\n",
      "Chunk 8 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:58<00:00,  8.51items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_009.csv\n",
      "Chunk 9 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:57<00:00,  8.73items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_010.csv\n",
      "Chunk 10 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:59<00:00,  8.42items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_011.csv\n",
      "Chunk 11 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:01<00:00,  8.07items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_012.csv\n",
      "Chunk 12 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:11<00:00,  6.96items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_013.csv\n",
      "Chunk 13 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:09<00:00,  7.23items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_014.csv\n",
      "Chunk 14 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:08<00:00,  7.25items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_015.csv\n",
      "Chunk 15 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:04<00:00,  7.79items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_016.csv\n",
      "Chunk 16 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:03<00:00,  7.82items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_017.csv\n",
      "Chunk 17 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:04<00:00,  7.80items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_018.csv\n",
      "Chunk 18 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:06<00:00,  7.57items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_019.csv\n",
      "Chunk 19 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:04<00:00,  7.76items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_020.csv\n",
      "Chunk 20 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:04<00:00,  7.77items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_021.csv\n",
      "Chunk 21 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:06<00:00,  7.51items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_022.csv\n",
      "Chunk 22 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:06<00:00,  7.56items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_023.csv\n",
      "Chunk 23 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:07<00:00,  7.41items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_024.csv\n",
      "Chunk 24 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:02<00:00,  7.94items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_025.csv\n",
      "Chunk 25 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:03<00:00,  7.86items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_026.csv\n",
      "Chunk 26 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:02<00:00,  7.98items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_027.csv\n",
      "Chunk 27 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:05<00:00,  7.64items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_028.csv\n",
      "Chunk 28 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:04<00:00,  7.69items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_029.csv\n",
      "Chunk 29 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:07<00:00,  7.39items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_030.csv\n",
      "Chunk 30 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:07<00:00,  7.38items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_031.csv\n",
      "Chunk 31 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:07<00:00,  7.37items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_032.csv\n",
      "Chunk 32 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:05<00:00,  7.59items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_033.csv\n",
      "Chunk 33 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:03<00:00,  7.91items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_034.csv\n",
      "Chunk 34 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:03<00:00,  7.82items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_035.csv\n",
      "Chunk 35 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:19<00:00,  6.25items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_036.csv\n",
      "Chunk 36 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:21<00:00,  6.15items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_037.csv\n",
      "Chunk 37 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:02<00:00,  7.94items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_038.csv\n",
      "Chunk 38 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:05<00:00,  7.61items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_039.csv\n",
      "Error log saved to ../data/data_extraction/GroupNode_errors.csv\n",
      "Chunk 39 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:10<00:00,  7.08items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_040.csv\n",
      "Error log saved to ../data/data_extraction/GroupNode_errors.csv\n",
      "Chunk 40 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:08<00:00,  7.33items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_041.csv\n",
      "Chunk 41 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:04<00:00,  7.69items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_042.csv\n",
      "Chunk 42 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:55<00:00,  9.00items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_043.csv\n",
      "Chunk 43 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:58<00:00,  8.58items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_044.csv\n",
      "Chunk 44 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:59<00:00,  8.42items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_045.csv\n",
      "Chunk 45 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:07<00:00,  7.41items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_046.csv\n",
      "Error log saved to ../data/data_extraction/GroupNode_errors.csv\n",
      "Chunk 46 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:09<00:00,  7.18items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_047.csv\n",
      "Chunk 47 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:05<00:00,  7.69items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_048.csv\n",
      "Chunk 48 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:16<00:00,  6.55items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_049.csv\n",
      "Chunk 49 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:07<00:00,  7.38items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_050.csv\n",
      "Chunk 50 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:04<00:00,  7.76items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_051.csv\n",
      "Chunk 51 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:14<00:00,  6.68items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_052.csv\n",
      "Chunk 52 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:05<00:00,  7.59items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_053.csv\n",
      "Chunk 53 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:12<00:00,  6.91items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_054.csv\n",
      "Error log saved to ../data/data_extraction/GroupNode_errors.csv\n",
      "Chunk 54 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:06<00:00,  7.58items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_055.csv\n",
      "Chunk 55 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:07<00:00,  7.42items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_056.csv\n",
      "Chunk 56 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:19<00:00,  6.31items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_057.csv\n",
      "Chunk 57 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:06<00:00,  7.47items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_058.csv\n",
      "Chunk 58 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:12<00:00,  6.87items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_059.csv\n",
      "Chunk 59 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:12<00:00,  6.85items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_060.csv\n",
      "Error log saved to ../data/data_extraction/GroupNode_errors.csv\n",
      "Chunk 60 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:01<00:00,  8.09items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_061.csv\n",
      "Chunk 61 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:04<00:00,  7.79items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_062.csv\n",
      "Chunk 62 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:04<00:00,  7.76items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_063.csv\n",
      "Chunk 63 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:03<00:00,  7.90items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_064.csv\n",
      "Chunk 64 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:04<00:00,  7.74items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_065.csv\n",
      "Chunk 65 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:13<00:00,  6.83items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_066.csv\n",
      "Chunk 66 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:02<00:00,  8.05items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_067.csv\n",
      "Chunk 67 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:25<00:00,  5.85items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_068.csv\n",
      "Chunk 68 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:20<00:00,  6.22items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_069.csv\n",
      "Error log saved to ../data/data_extraction/GroupNode_errors.csv\n",
      "Chunk 69 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:08<00:00,  7.25items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_070.csv\n",
      "Chunk 70 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:22<00:00,  6.08items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_071.csv\n",
      "Chunk 71 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:15<00:00,  6.64items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_072.csv\n",
      "Chunk 72 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:09<00:00,  7.17items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_073.csv\n",
      "Chunk 73 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:17<00:00,  6.49items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_074.csv\n",
      "Chunk 74 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:02<00:00,  8.04items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_075.csv\n",
      "Chunk 75 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:10<00:00,  7.04items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_076.csv\n",
      "Chunk 76 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:58<00:00,  8.49items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_077.csv\n",
      "Chunk 77 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:12<00:00,  6.94items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_078.csv\n",
      "Chunk 78 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:28<00:00,  5.65items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_079.csv\n",
      "Chunk 79 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:10<00:00,  7.09items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_080.csv\n",
      "Chunk 80 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:08<00:00,  7.27items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_081.csv\n",
      "Chunk 81 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:27<00:00,  5.72items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_082.csv\n",
      "Chunk 82 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:11<00:00,  6.99items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_083.csv\n",
      "Chunk 83 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:11<00:00,  6.96items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_084.csv\n",
      "Chunk 84 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:18<00:00,  6.34items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_085.csv\n",
      "Chunk 85 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:07<00:00,  7.45items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_086.csv\n",
      "Chunk 86 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:02<00:00,  7.98items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_087.csv\n",
      "Chunk 87 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:20<00:00,  6.18items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_088.csv\n",
      "Chunk 88 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:09<00:00,  7.25items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_089.csv\n",
      "Chunk 89 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:12<00:00,  6.92items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_090.csv\n",
      "Chunk 90 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:25<00:00,  5.83items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_091.csv\n",
      "Chunk 91 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:12<00:00,  6.86items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_092.csv\n",
      "Chunk 92 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:19<00:00,  6.27items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_093.csv\n",
      "Chunk 93 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:03<00:00,  7.81items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_094.csv\n",
      "Chunk 94 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:08<00:00,  7.26items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_095.csv\n",
      "Chunk 95 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:06<00:00,  7.51items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_096.csv\n",
      "Chunk 96 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:23<00:00,  5.98items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_097.csv\n",
      "Chunk 97 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:11<00:00,  7.01items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_098.csv\n",
      "Chunk 98 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:26<00:00,  5.80items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_099.csv\n",
      "Chunk 99 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:06<00:00,  7.50items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_100.csv\n",
      "Chunk 100 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:11<00:00,  6.96items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_101.csv\n",
      "Chunk 101 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:25<00:00,  5.86items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_102.csv\n",
      "Chunk 102 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:31<00:00,  5.47items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_103.csv\n",
      "Chunk 103 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:05<00:00,  7.62items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_104.csv\n",
      "Chunk 104 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:22<00:00,  6.05items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_105.csv\n",
      "Chunk 105 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:10<00:00,  7.10items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_106.csv\n",
      "Chunk 106 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:21<00:00,  6.13items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_107.csv\n",
      "Chunk 107 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:32<00:00,  5.42items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_108.csv\n",
      "Chunk 108 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:20<00:00,  6.19items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_109.csv\n",
      "Chunk 109 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:02<00:00,  7.99items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_110.csv\n",
      "Chunk 110 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:00<00:00,  8.22items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_111.csv\n",
      "Chunk 111 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:58<00:00,  8.51items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_112.csv\n",
      "Error log saved to ../data/data_extraction/GroupNode_errors.csv\n",
      "Chunk 112 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [00:59<00:00,  8.36items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_113.csv\n",
      "Error log saved to ../data/data_extraction/GroupNode_errors.csv\n",
      "Chunk 113 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:20<00:00,  6.24items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_114.csv\n",
      "Chunk 114 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:12<00:00,  6.89items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_115.csv\n",
      "Chunk 115 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:19<00:00,  6.25items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_116.csv\n",
      "Chunk 116 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:13<00:00,  6.83items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_117.csv\n",
      "Chunk 117 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:05<00:00,  7.61items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_118.csv\n",
      "Chunk 118 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:13<00:00,  6.82items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_119.csv\n",
      "Chunk 119 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:23<00:00,  6.02items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_120.csv\n",
      "Chunk 120 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:07<00:00,  7.38items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_121.csv\n",
      "Chunk 121 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:05<00:00,  7.60items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_122.csv\n",
      "Chunk 122 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:19<00:00,  6.25items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_123.csv\n",
      "Chunk 123 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:06<00:00,  7.53items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_124.csv\n",
      "Chunk 124 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:17<00:00,  6.44items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_125.csv\n",
      "Chunk 125 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:00<00:00,  8.20items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_126.csv\n",
      "Chunk 126 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:06<00:00,  7.51items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_127.csv\n",
      "Chunk 127 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 500/500 [01:08<00:00,  7.29items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_128.csv\n",
      "Chunk 128 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|███████████████████| 373/373 [01:00<00:00,  6.19items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk to ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_129.csv\n",
      "Chunk 129 processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from progress_tracker import track_progress\n",
    "from web_data_extraction import create_robust_session\n",
    "\n",
    "\n",
    "def replace_extension(group_link, new_extension=\"html\"):\n",
    "    if not isinstance(group_link, str) or not group_link.endswith('.json'):\n",
    "        return None\n",
    "    return group_link.replace('.json', f'.{new_extension}')\n",
    "\n",
    "\n",
    "def parse_html_group_table(html_content, survey_id, survey_group_id):\n",
    "    \"\"\"Parse HTML content containing a group table and extract relevant fields.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the table\n",
    "    table = soup.find('table')\n",
    "    if table is None:\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "    \n",
    "    # Initialize lists for data collection\n",
    "    data = []\n",
    "    \n",
    "    # Extract headers\n",
    "    headers = [th.text.strip() for th in table.find_all('th')]\n",
    "    if not headers:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Add additional header for the variable link\n",
    "    if \"Name\" in headers:\n",
    "        name_index = headers.index(\"Name\")\n",
    "        headers.insert(name_index + 1, \"Variable Link\")  # Add a column for the link\n",
    "\n",
    "    # Dynamically adjust headers to match the longest row\n",
    "    max_columns = max(len(row.find_all('td')) for row in table.find_all('tr') if row.find_all('td'))\n",
    "    if len(headers) < max_columns:\n",
    "        headers += [f\"Extra_Column_{i}\" for i in range(len(headers), max_columns)]\n",
    "\n",
    "    # Extract rows\n",
    "    for row in table.find_all('tr')[1:]:  # Skip header row\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) < len(headers) - 1:\n",
    "            # Skip obviously malformed rows\n",
    "            continue\n",
    "        \n",
    "        row_data = []\n",
    "        for i, cell in enumerate(cells):\n",
    "            # Handle the Name column (extract text and link)\n",
    "            if i == name_index:\n",
    "                link = cell.find('a')\n",
    "                if link:\n",
    "                    row_data.append(link.text.strip())  # Variable Name\n",
    "                    row_data.append(link.get('href', ''))  # Variable Link\n",
    "                else:\n",
    "                    row_data.append(cell.text.strip())  # Add name even if no link\n",
    "                    row_data.append(None)  # No link available\n",
    "            else:\n",
    "                row_data.append(cell.text.strip())\n",
    "\n",
    "        if len(row_data) < len(headers):\n",
    "            row_data += [None] * (len(headers) - len(row_data))  # Pad missing columns\n",
    "        \n",
    "        data.append(row_data)\n",
    "\n",
    "    if not data:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Create DataFrame\n",
    "    try:\n",
    "        df = pd.DataFrame(data, columns=headers[:len(data[0])])  # Align columns dynamically\n",
    "        # Add metadata columns\n",
    "        df['SurveyID'] = survey_id\n",
    "        df['SurveyGroupID'] = survey_group_id\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataFrame: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def process_group_htmls(survey_group_df, output_chunk_csv, error_log_csv, session):\n",
    "    \"\"\"Process group HTMLs and save results.\"\"\"\n",
    "    all_group_data = []\n",
    "    error_log = []\n",
    "    processed_rows = 0\n",
    "\n",
    "    for _, row in track_progress(survey_group_df.iterrows(), \n",
    "                               description=\"Processing Groups\", \n",
    "                               total=survey_group_df.shape[0]):\n",
    "        \n",
    "        group_link = replace_extension(row['GroupLink'])\n",
    "        if not group_link:\n",
    "            error_log.append({\n",
    "                \"SurveyGroupID\": row['SurveyGroupID'],\n",
    "                \"SurveyID\": row['SurveyID'],\n",
    "                \"GroupLink\": row['GroupLink'],\n",
    "                \"Error\": \"Invalid GroupLink\"\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Fetch HTML\n",
    "            response = session.get(group_link)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse table\n",
    "            group_data = parse_html_group_table(\n",
    "                response.text,\n",
    "                row['SurveyID'],\n",
    "                row['SurveyGroupID']\n",
    "            )\n",
    "            \n",
    "            if not group_data.empty:\n",
    "                all_group_data.append(group_data)\n",
    "                processed_rows += 1\n",
    "            else:\n",
    "                error_log.append({\n",
    "                    \"SurveyGroupID\": row['SurveyGroupID'],\n",
    "                    \"SurveyID\": row['SurveyID'],\n",
    "                    \"GroupLink\": group_link,\n",
    "                    \"Error\": \"No data rows found\"\n",
    "                })\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log error and stop processing\n",
    "            error_log.append({\n",
    "                \"SurveyGroupID\": row['SurveyGroupID'],\n",
    "                \"SurveyID\": row['SurveyID'],\n",
    "                \"GroupLink\": group_link,\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "            print(f\"Error encountered. Stopping processing.\")\n",
    "            break\n",
    "\n",
    "    # Save all collected data to the single output CSV\n",
    "    if all_group_data:\n",
    "        # Concatenate all collected dataframes\n",
    "        final_df = pd.concat(all_group_data, ignore_index=True)\n",
    "        final_df.to_csv(output_chunk_csv, index=False)\n",
    "        print(f\"Saved chunk to {output_chunk_csv}\")\n",
    "\n",
    "    # Save error log\n",
    "    if error_log:\n",
    "        pd.DataFrame(error_log).to_csv(error_log_csv, index=False, mode='a')\n",
    "        print(f\"Error log saved to {error_log_csv}\")\n",
    "\n",
    "\n",
    "def save_chunk(data, output_folder, chunk_index):\n",
    "    \"\"\"Save a chunk of processed data to a CSV file.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    chunk_path = os.path.join(output_folder, f\"GroupNode_{chunk_index:03d}.csv\")\n",
    "    pd.concat(data, ignore_index=True).to_csv(chunk_path, index=False)\n",
    "    print(f\"Saved chunk {chunk_index} to {chunk_path}\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def get_resume_start_index(output_folder, survey_group_df):\n",
    "    \"\"\"\n",
    "    Determine the starting index for resumption by checking existing chunks.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    output_folder : str\n",
    "        Path to the folder where output chunks are stored.\n",
    "    survey_group_df : pd.DataFrame\n",
    "        Input DataFrame containing the full dataset.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Starting index for resumption.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for existing files in the output folder\n",
    "    chunk_files = [f for f in os.listdir(output_folder) if f.startswith(\"GroupNode_chunk_\") and f.endswith(\".csv\")]\n",
    "    \n",
    "    if not chunk_files:\n",
    "        # No files processed yet; start from the beginning\n",
    "        return 0\n",
    "    \n",
    "    # Extract the last processed chunk number\n",
    "    chunk_numbers = [\n",
    "        int(re.search(r\"GroupNode_chunk_(\\d+).csv\", file).group(1))\n",
    "        for file in chunk_files\n",
    "        if re.search(r\"GroupNode_chunk_(\\d+).csv\", file)\n",
    "    ]\n",
    "    \n",
    "    if not chunk_numbers:\n",
    "        return 0  # Start from the beginning if no valid chunk files are found\n",
    "    \n",
    "    # Find the maximum chunk number and calculate start index\n",
    "    last_chunk_number = max(chunk_numbers)\n",
    "    last_chunk_size = len(pd.read_csv(os.path.join(output_folder, f\"GroupNode_chunk_{last_chunk_number:03d}.csv\")))\n",
    "    start_index = last_chunk_number * chunk_size + last_chunk_size\n",
    "    \n",
    "    print(f\"Resuming from index {start_index}, skipping {last_chunk_number} completed chunks.\")\n",
    "    return start_index\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create session\n",
    "    session = create_robust_session()\n",
    "    \n",
    "    # Paths\n",
    "    input_csv = '../data/data_extraction/SurveyGroupNode.csv'\n",
    "    output_folder = '../data/data_extraction/GroupNodeChunks'\n",
    "    error_log_csv = '../data/data_extraction/GroupNode_errors.csv'\n",
    "    \n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Process input data in chunks\n",
    "        chunk_size = 500\n",
    "        chunk_index = 0\n",
    "        \n",
    "        for chunk in pd.read_csv(input_csv, chunksize=chunk_size):\n",
    "            # Define chunk-specific output path\n",
    "            output_chunk_csv = os.path.join(output_folder, f'GroupNode_chunk_{chunk_index:03d}.csv')\n",
    "            \n",
    "            # Skip processing if the chunk already exists\n",
    "            if os.path.exists(output_chunk_csv):\n",
    "                print(f\"Chunk {chunk_index} already processed. Skipping.\")\n",
    "                chunk_index += 1\n",
    "                continue\n",
    "        \n",
    "            # Process the chunk\n",
    "            process_group_htmls(chunk, output_chunk_csv, error_log_csv, session)\n",
    "            print(f\"Chunk {chunk_index} processed successfully.\")\n",
    "            \n",
    "            chunk_index += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading input file in chunks: {str(e)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2663f7e-2c76-43a5-917b-fd0e234f3c64",
   "metadata": {},
   "source": [
    "## 3.5.1.1 Process the Error Log -- Special Suppliment: Workflow for Cyclic Processing of Errors\n",
    "\n",
    "- Despite best efforts, not all files will process and this necessitates itterating the above process.\n",
    "- Save off all itterations of the error file as backup/evidence (Cycle1..CycleN folder)\n",
    "- Manually add the additional batchs, increment as appropriate to the GroupNode folder\n",
    "- Worst case scenario is manual download to avoid whatever is causing the issue\n",
    "\n",
    "  >NOTE: It took four cycles to get everything on the original run.\n",
    "\n",
    "\n",
    "### **Step-by-Step Guide**\n",
    "\n",
    "### **1. Initial Run:**\n",
    "- Place your raw `GroupNode_errors.csv` file in the specified `error_file` path.\n",
    "- Run the script:\n",
    "  - The script will clean the error log and process the cleaned data in chunks.\n",
    "  - Results are saved in the `Reprocessed_GroupNodeChunks` folder.\n",
    "  - Errors from processing are logged into a new `Reprocessed_GroupNode_errors.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Subsequent Runs for Remaining Errors:**\n",
    "- Use the latest `Reprocessed_GroupNode_errors.csv` as your new `error_file`.\n",
    "- Repeat the script:\n",
    "  - Clean this new error file.\n",
    "  - Process the cleaned data.\n",
    "  - Save processed chunks and log new errors.\n",
    "- **Verify Results:**\n",
    "  - Confirm that the number of rows in the error file decreases with each iteration.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Final Check:**\n",
    "- Keep cycling the error file through the script until:\n",
    "  - The `Reprocessed_GroupNode_errors.csv` file is either empty or contains only truly invalid records (e.g., broken links, malformed tables).\n",
    "- The final consolidated data will reside in the `Reprocessed_GroupNodeChunks` folder.\n",
    "\n",
    "---\n",
    "\n",
    "## **Plan for Edge Case Handling**\n",
    "\n",
    "### **Duplicate Records in Error File:**\n",
    "- If the same records appear repeatedly in the error log despite being processed, investigate:\n",
    "  - Table structure changes in those HTML files.\n",
    "  - Potential issues with URL accessibility (e.g., timeouts, restrictions).\n",
    "\n",
    "---\n",
    "\n",
    "### **Stuck or Missing Rows:**\n",
    "- Manually inspect `Reprocessed_GroupNode_errors.csv` for persistent errors.\n",
    "- Validate the URLs or files manually to confirm if they are genuinely problematic.\n",
    "\n",
    "---\n",
    "\n",
    "## **Additional Tip for Manual Validations:**\n",
    "To streamline manual inspections:\n",
    "1. Use a browser or Postman to validate individual URLs directly from the error file.\n",
    "2. Fix any recurring issues in the HTML parser (`parse_html_group_table`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "968f985f-159e-4469-b893-55963b71203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned error log saved to ../data/data_extraction/Cleaned_GroupNode_errors.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunk 0: 100%|██████████████████████| 1/1 [00:00<00:00,  3.40items/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 processed successfully. Saved to ../data/data_extraction/Reprocessed_GroupNodeChunks/Reprocessed_GroupNode_chunk_000.csv\n",
      "Reprocessing completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from progress_tracker import track_progress\n",
    "\n",
    "\n",
    "def create_robust_session():\n",
    "    session = requests.Session()\n",
    "    session.headers.update({'User-Agent': 'Mozilla/5.0'})\n",
    "    return session\n",
    "\n",
    "\n",
    "def parse_html_group_table(html_content, survey_id, survey_group_id):\n",
    "    \"\"\"\n",
    "    Parse HTML content containing a group table and extract relevant fields.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    table = soup.find('table')\n",
    "    if not table:\n",
    "        print(f\"No table found for SurveyGroupID {survey_group_id}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extract headers\n",
    "    header_row = table.find('tr')\n",
    "    headers = [th.text.strip() for th in header_row.find_all(['th', 'td'])] if header_row else []\n",
    "    if not headers:\n",
    "        print(f\"No headers found for SurveyGroupID {survey_group_id}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extract rows\n",
    "    rows = []\n",
    "    for row in table.find_all('tr')[1:]:  # Skip header row\n",
    "        cells = row.find_all('td')\n",
    "        if cells:\n",
    "            row_data = []\n",
    "            variable_link = None  # Placeholder for Variable Link column\n",
    "            for cell in cells:\n",
    "                link = cell.find('a')\n",
    "                if link:\n",
    "                    row_data.append(cell.text.strip())  # Add variable name\n",
    "                    variable_link = link.get('href', '')  # Capture URL\n",
    "                else:\n",
    "                    row_data.append(cell.text.strip())\n",
    "            if variable_link:\n",
    "                row_data.append(variable_link)  # Append Variable Link at the end\n",
    "            rows.append(row_data)\n",
    "\n",
    "    # Ensure headers match expanded rows\n",
    "    if any(len(row) > len(headers) for row in rows):\n",
    "        headers += [\"Variable Link\"]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    df['SurveyID'] = survey_id\n",
    "    df['SurveyGroupID'] = survey_group_id\n",
    "\n",
    "    # Drop rows where both 'Group' and 'Label' are blank\n",
    "    df = df[~(df['Group'].isna() & df['Label'].isna())]\n",
    "\n",
    "    # Reorder columns as needed\n",
    "    desired_column_order = [\n",
    "        \"Name\", \"Variable Link\", \"Label\", \"Concept\", \"Required\", \"Attributes\",\n",
    "        \"Limit\", \"Predicate Type\", \"Group\", \"SurveyID\", \"SurveyGroupID\"\n",
    "    ]\n",
    "    existing_columns = [col for col in desired_column_order if col in df.columns]\n",
    "    remaining_columns = [col for col in df.columns if col not in existing_columns]\n",
    "    df = df[existing_columns + remaining_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def reprocess_cleaned_data(cleaned_file, output_folder, session, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Reprocess cleaned data from a consolidated file.\n",
    "    \"\"\"\n",
    "    cleaned_df = pd.read_csv(cleaned_file)\n",
    "    all_group_data = []\n",
    "    errors = []\n",
    "\n",
    "    for chunk_start in range(0, len(cleaned_df), chunk_size):\n",
    "        chunk = cleaned_df.iloc[chunk_start:chunk_start + chunk_size]\n",
    "        chunk_index = chunk_start // chunk_size\n",
    "\n",
    "        output_chunk_csv = os.path.join(output_folder, f\"Reprocessed_GroupNode_chunk_{chunk_index:03d}.csv\")\n",
    "        error_chunk_log = os.path.join(output_folder, f\"Reprocessed_GroupNode_errors_chunk_{chunk_index:03d}.csv\")\n",
    "\n",
    "        chunk_group_data = []\n",
    "        chunk_errors = []\n",
    "\n",
    "        for _, row in track_progress(chunk.iterrows(), description=f\"Processing Chunk {chunk_index}\", total=len(chunk)):\n",
    "            try:\n",
    "                group_link = row['GroupLink']\n",
    "                survey_group_id = row['SurveyGroupID']\n",
    "                survey_id = row['SurveyID']\n",
    "\n",
    "                response = session.get(group_link)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                group_data = parse_html_group_table(response.text, survey_id, survey_group_id)\n",
    "                if not group_data.empty:\n",
    "                    chunk_group_data.append(group_data)\n",
    "                else:\n",
    "                    chunk_errors.append(row.to_dict())\n",
    "            except Exception as e:\n",
    "                chunk_errors.append({**row.to_dict(), \"Error\": str(e)})\n",
    "\n",
    "        # Save chunk results\n",
    "        if chunk_group_data:\n",
    "            pd.concat(chunk_group_data, ignore_index=True).to_csv(output_chunk_csv, index=False)\n",
    "            print(f\"Chunk {chunk_index} processed successfully. Saved to {output_chunk_csv}\")\n",
    "\n",
    "        # Save errors for the chunk\n",
    "        if chunk_errors:\n",
    "            pd.DataFrame(chunk_errors).to_csv(error_chunk_log, index=False)\n",
    "            print(f\"Errors logged to {error_chunk_log}\")\n",
    "\n",
    "    print(\"Reprocessing completed successfully!\")\n",
    "\n",
    "\n",
    "def clean_error_file(error_file, cleaned_output_file):\n",
    "    \"\"\"\n",
    "    Cleans an error file by removing duplicated header rows and rows with missing or invalid data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(error_file)\n",
    "\n",
    "        # Remove duplicate headers and rows missing critical columns\n",
    "        df = df[~df.apply(lambda x: x.str.contains(\"SurveyGroupID|SurveyID|GroupLink|Error\", na=False).all(), axis=1)]\n",
    "        df = df.dropna(subset=[\"GroupLink\", \"SurveyGroupID\"])\n",
    "        df = df[df['GroupLink'].str.startswith(\"http\", na=False)]\n",
    "\n",
    "        df.to_csv(cleaned_output_file, index=False)\n",
    "        print(f\"Cleaned error log saved to {cleaned_output_file}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning file {error_file}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define file paths\n",
    "    error_file = \"../data/data_extraction/GroupNode_errors.csv\"\n",
    "    cleaned_error_file = \"../data/data_extraction/Cleaned_GroupNode_errors.csv\"\n",
    "    output_folder = \"../data/data_extraction/Reprocessed_GroupNodeChunks\"\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Step 1: Clean the error file\n",
    "    cleaned_df = clean_error_file(error_file, cleaned_error_file)\n",
    "    if cleaned_df.empty:\n",
    "        print(\"No valid rows to process after cleaning. Exiting.\")\n",
    "        exit(0)\n",
    "\n",
    "    # Step 2: Create session\n",
    "    session = create_robust_session()\n",
    "\n",
    "    # Step 3: Reprocess cleaned data\n",
    "    reprocess_cleaned_data(cleaned_error_file, output_folder, session, chunk_size=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6195e-6140-4f5f-8f04-61917eceeeba",
   "metadata": {},
   "source": [
    "## 3.5.1.2 Supplimentary Error Processing Log\n",
    "\n",
    "- the URL processing for 'Variable Link' inadvertently stored the group URL\n",
    "- Rather than reprocess everything, and JIC its an issue elsewhere, we'll just fix as a batch\n",
    "- Something like this might be useful for more URL fixing at some point and we can reuse code from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "25c0c19f-1295-4ac1-bbc1-4227170ce930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to ../data/data_extraction/Group_url_fix/GroupNode_chunk_130.csv\n",
      "Processed file saved to ../data/data_extraction/Group_url_fix/GroupNode_chunk_131.csv\n",
      "Processed file saved to ../data/data_extraction/Group_url_fix/GroupNode_chunk_132.csv\n",
      "Processed file saved to ../data/data_extraction/Group_url_fix/GroupNode_chunk_133.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def fix_group_url(url, name):\n",
    "    \"\"\"\n",
    "    Fixes a group URL to point to the correct variable URL.\n",
    "    \"\"\"\n",
    "    if \"/groups/\" in url and url.endswith(\".html\"):\n",
    "        base_url = url.split('/groups/')[0]  # Get everything before '/groups/'\n",
    "        return f\"{base_url}/variables/{name}.json\"\n",
    "    return url\n",
    "\n",
    "def batch_url_transformation(input_files, output_folder):\n",
    "    \"\"\"\n",
    "    Processes the Variable Link column in the given batch of files,\n",
    "    applying the fix_group_url logic and saving the result.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for chunk_file in input_files:\n",
    "        # Load the chunk file\n",
    "        df = pd.read_csv(chunk_file)\n",
    "\n",
    "        # Transform and replace the Variable Link column\n",
    "        df['Variable Link'] = df.apply(\n",
    "            lambda row: fix_group_url(row['Variable Link'], row['Name']), axis=1\n",
    "        )\n",
    "\n",
    "        # Prepare output file path\n",
    "        output_file = os.path.join(\n",
    "            output_folder,\n",
    "            os.path.basename(chunk_file)\n",
    "        )\n",
    "\n",
    "        # Save the updated DataFrame\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed file saved to {output_file}\")\n",
    "\n",
    "# Define input files and output folder\n",
    "input_files = [\n",
    "    \"../data/data_extraction/GroupNodeChunks/GroupNode_chunk_130.csv\",\n",
    "    \"../data/data_extraction/GroupNodeChunks/GroupNode_chunk_131.csv\",\n",
    "    \"../data/data_extraction/GroupNodeChunks/GroupNode_chunk_132.csv\",\n",
    "    \"../data/data_extraction/GroupNodeChunks/GroupNode_chunk_133.csv\"\n",
    "]\n",
    "output_folder = \"../data/data_extraction/Group_url_fix/\"\n",
    "\n",
    "# Run the batch processing\n",
    "batch_url_transformation(input_files, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c80293-d910-466c-8bb6-819e7e40a91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "587f78f2-5f01-40ca-a80b-9aceb2907e3e",
   "metadata": {},
   "source": [
    "## 3.5.2 Processes each Variable JSON file in the GroupNode Table\n",
    "- Reminder: variable JSON file contains `attribues` which define directional relationships with other variables.\n",
    "- **Goal**:get the attributes and add the relationship as extra columns\n",
    "\n",
    "**NOTE** Single file processing was too time consuming. A python script was created to launch 5 concurent sessions and extract all the data. This process, errors nonwidthstanding, took 3.25 days. More detail is in Section 3.5.2.2 below.\n",
    "\n",
    "## Documentation: Processing Census GroupNode Chunks\n",
    "\n",
    "## Overview\n",
    "This process handles the sequential processing of Census GroupNode chunks stored in CSV format. The data includes variable links pointing to JSON files containing additional metadata. The goal is to enhance the dataset by:\n",
    "1. Constructing full URLs for the JSON files.\n",
    "2. Fixing any inconsistencies in file extensions.\n",
    "3. Extracting metadata from the JSON files and appending it to the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Logic\n",
    "\n",
    "### 1. Read Data Sequentially\n",
    "- Load each chunk file from the `GroupNodeChunks` folder in sequence.\n",
    "- Use a loop to iterate through all the files stored in the folder.\n",
    "\n",
    "### 2. Construct Full URLs\n",
    "- Census provides variable links in a relative path format (e.g., `/data/2000/dec/aian/variables/HCT110005.json`).\n",
    "- Prepend the base URL `https://api.census.gov` to each relative path in the `Variable Link` column.\n",
    "- Replace `.html` extensions with `.json` if any URLs are incorrectly formatted.\n",
    "\n",
    "### 3. Add New Columns\n",
    "- Add the following columns to the dataset with default values:\n",
    "  - **`Has Attribute`**: Default to `'no attribute'`. Updated to `'has attribute'` if the JSON file contains the `attributes` field.\n",
    "  - **`Attribute of`**: Default to `'NA'`. Updated to the value of the `attribute of` field if present in the JSON file.\n",
    "  - **`Attribute Type`**: Default to `'NA'`. Updated to the value of the `attribute type` field if present in the JSON file.\n",
    "\n",
    "### 4. Process Each Row\n",
    "- For each row in the dataset:\n",
    "  1. Fetch the JSON file from the `Variable Link` URL using an HTTP GET request.\n",
    "  2. Parse the JSON response and extract relevant fields:\n",
    "     - **`attributes`**: If present, update the `Has Attribute` column to `'has attribute'` and populate the `attributes` column with the value.\n",
    "     - **`attribute of`**: If present, populate the `Attribute of` column.\n",
    "     - **`attribute type`**: If present, populate the `Attribute Type` column.\n",
    "  3. Handle any errors gracefully and log them for debugging.\n",
    "\n",
    "### 5. Save the Processed File\n",
    "- Save the updated dataset to a new CSV file in the `Processed_GroupNodeChunks` folder.\n",
    "\n",
    "---\n",
    "\n",
    "## Example JSON File\n",
    "```json\n",
    "{\n",
    "  \"name\": \"DP02_0001PM\",\n",
    "  \"label\": \"Percent!!Margin of Error!!HOUSEHOLDS BY TYPE!!Total households\",\n",
    "  \"concept\": \"Selected Social Characteristics in the United States: 2008\",\n",
    "  \"predicateType\": \"int\",\n",
    "  \"group\": \"DP02\",\n",
    "  \"limit\": 0,\n",
    "  \"attributes\": \"DP02_0001PMA\",\n",
    "  \"attribute of\": \"DP02_0001PE\",\n",
    "  \"attribute type\": \"Margin of Error\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97712d48-4e68-4e5e-b7a6-ae4dba3434f1",
   "metadata": {},
   "source": [
    "## 3.5.2.1 Single Chunk Example for Demonstrating Functionality \n",
    "\n",
    "```\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from progress_tracker import track_progress\n",
    "\n",
    "# Create robust session\n",
    "def create_robust_session():\n",
    "    session = requests.Session()\n",
    "    session.headers.update({'User-Agent': 'Mozilla/5.0'})\n",
    "    return session\n",
    "\n",
    "# Process one chunk file\n",
    "def process_groupnode_chunk(chunk_file, output_folder):\n",
    "    base_url = \"https://api.census.gov\"\n",
    "    session = create_robust_session()\n",
    "    \n",
    "    # Load the chunk\n",
    "    df = pd.read_csv(chunk_file)\n",
    "    \n",
    "    # Rename 'Name' column to 'Variable Name'\n",
    "    df.rename(columns={'Name': 'Variable Name'}, inplace=True)\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Prepare paths for output\n",
    "    chunk_name = os.path.splitext(os.path.basename(chunk_file))[0]\n",
    "    output_file = os.path.join(output_folder, f\"Processed_{chunk_name}.csv\")\n",
    "    error_log_file = os.path.join(output_folder, f\"Errors_{chunk_name}.csv\")\n",
    "\n",
    "    # Ensure the full URL and clean extensions\n",
    "    df['Variable Link'] = df['Variable Link'].apply(lambda x: f\"{base_url}{x}\" if x.startswith('/data') else x)\n",
    "    df['Variable Link'] = df['Variable Link'].str.replace('.html', '.json')\n",
    "\n",
    "    # Ensure existing columns have the correct dtype\n",
    "    df['Attributes'] = df['Attributes'].astype('object')\n",
    "    if 'Attribute Of' not in df.columns:\n",
    "        df['Attribute Of'] = pd.Series(dtype='object')\n",
    "    if 'Attribute Type' not in df.columns:\n",
    "        df['Attribute Type'] = pd.Series(dtype='object')\n",
    "\n",
    "    errors = []  # To store error information\n",
    "\n",
    "    # Process each variable\n",
    "    for i, row in track_progress(df.iterrows(), description=f\"Processing {chunk_name}\", total=len(df)):\n",
    "        variable_url = row['Variable Link']\n",
    "        \n",
    "        try:\n",
    "            # Request the JSON\n",
    "            response = session.get(variable_url)\n",
    "            response.raise_for_status()\n",
    "            variable_data = response.json()\n",
    "            \n",
    "            # Update columns based on JSON content\n",
    "            df.at[i, 'Attributes'] = variable_data.get('attributes', 'na')\n",
    "            df.at[i, 'Attribute Of'] = variable_data.get('attribute of', 'na')\n",
    "            df.at[i, 'Attribute Type'] = variable_data.get('attribute type', 'na')\n",
    "        except Exception as e:\n",
    "            # Capture errors in the log\n",
    "            errors.append({\n",
    "                \"Index\": i,\n",
    "                \"Variable Name\": row.get('Variable Name', 'na'),\n",
    "                \"Group\": row.get('Group', 'na'),\n",
    "                \"Variable Link\": variable_url,\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "\n",
    "    # Reorder columns\n",
    "    column_order = [\n",
    "        \"SurveyID\", \"SurveyGroupID\", \"Group\", \"Variable Name\", \"Variable Link\",\n",
    "        \"Label\", \"Concept\", \"Required\", \"Attributes\", \"Attribute Of\", \n",
    "        \"Attribute Type\", \"Limit\", \"Predicate Type\"   \n",
    "    ]\n",
    "    df = df[column_order]\n",
    "    \n",
    "    # Save the processed DataFrame\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed chunk saved to {output_file}\")\n",
    "    \n",
    "    # Save the error log if there are any errors\n",
    "    if errors:\n",
    "        error_df = pd.DataFrame(errors)\n",
    "        error_df.to_csv(error_log_file, index=False)\n",
    "        print(f\"Error log saved to {error_log_file}\")\n",
    "\n",
    "# Batch processing\n",
    "def process_all_chunks(input_folder, output_folder, start_chunk=0):\n",
    "    # Get a list of all files in the directory\n",
    "    all_files = sorted(\n",
    "        [f for f in os.listdir(input_folder) if f.startswith(\"GroupNode_chunk_\") and f.endswith(\".csv\")]\n",
    "    )\n",
    "    \n",
    "    # Sort files by their numeric chunk number\n",
    "    all_files = sorted(all_files, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "    # Filter files to start from the specified chunk\n",
    "    files_to_process = [\n",
    "        os.path.join(input_folder, f) for f in all_files\n",
    "        if int(f.split(\"_\")[-1].split(\".\")[0]) >= start_chunk\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(files_to_process)} files to process starting from chunk {start_chunk}.\")\n",
    "\n",
    "    # Process each file\n",
    "    for chunk_file in files_to_process:\n",
    "        process_groupnode_chunk(chunk_file, output_folder)\n",
    "\n",
    "# Example: Process all chunks starting from a specific number\n",
    "input_folder = \"../data/data_extraction/GroupNodeChunks\"\n",
    "output_folder = \"../data/data_extraction/ValidationSet-GroupNodesWithVariables\"\n",
    "start_chunk = 133  # Change this to the chunk number to start from (e.g., 132)\n",
    "\n",
    "process_all_chunks(input_folder, output_folder, start_chunk)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a70391-4f64-453b-bf64-f77da068753e",
   "metadata": {},
   "source": [
    "# We used the multi-core processing rather than single file ...\n",
    "## Discuss file in /src to explain how to use\n",
    "## Additional processing for error file logs still needed. \n",
    "\n",
    "## Error File Processing Script for Errors in Multi-Core Processing Process\n",
    "\n",
    "This script processes an error file for a specific chunk of the main table, fetches missing data using the `Variable Link`, and updates the main table with corrected values.\n",
    "\n",
    "## Key Features\n",
    "- **Dynamic Chunk Selection**: Specify the chunk number (`chunk_number`) to process the corresponding error file and main table.\n",
    "- **Data Fetching and Updates**: Retrieves JSON data from the `Variable Link` in the error file and updates the `Attributes`, `Attribute Of`, and `Attribute Type` columns in the main table.\n",
    "- **Error Logging**: Logs all unresolved errors into a new error file named `<error_file>--RemainingToFix.csv` for retrying later.\n",
    "- **Output Files**:\n",
    "  - The updated main table is saved as `<chunk_file>-FIXED.csv`.\n",
    "  - Any unresolved errors are saved for future processing.\n",
    "\n",
    "This ensures incremental error correction while maintaining logs for unresolved issues.\n",
    "\n",
    "**NOTE** This script was run one error file at a time, all error files and logs are saved to another directory for historical purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26b62d-05e7-46f9-a10c-e2088619ae00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Errors (Chunk 065):  45%|▍| 20357/45214 [1:45:07<2:01:12,  3.42items/"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from progress_tracker import track_progress\n",
    "from web_data_extraction import create_robust_session\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"error_processing.log\",\n",
    "    level=logging.ERROR,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Specify the chunk number to process\n",
    "chunk_number = \"065\"\n",
    "\n",
    "# File paths\n",
    "main_table_path = f\"../data/data_extraction/GroupNodesWithVariables/Processed_GroupNode_chunk_{chunk_number}.csv\"\n",
    "error_file_path = f\"../data/data_extraction/GroupNodesWithVariables/Errors_GroupNode_chunk_{chunk_number}.csv\"\n",
    "updated_main_table_path = f\"../data/data_extraction/GroupNodesWithVariables/Processed_GroupNode_chunk_{chunk_number}-FIXED.csv\"\n",
    "remaining_errors_path = f\"../data/data_extraction/GroupNodesWithVariables/Errors_GroupNode_chunk_{chunk_number}--RemainingToFix.csv\"\n",
    "\n",
    "# Load the tables\n",
    "main_df = pd.read_csv(main_table_path)\n",
    "errors_df = pd.read_csv(error_file_path)\n",
    "\n",
    "# Create robust session\n",
    "session = create_robust_session()\n",
    "\n",
    "# Track remaining errors\n",
    "remaining_errors = []\n",
    "\n",
    "# Process the error file\n",
    "for i, row in track_progress(errors_df.iterrows(), description=f\"Processing Errors (Chunk {chunk_number})\", total=len(errors_df)):\n",
    "    variable_link = row['Variable Link']\n",
    "    variable_name = row['Variable Name']\n",
    "    index = row['Index']\n",
    "    \n",
    "    try:\n",
    "        # Fetch JSON data from the Variable Link\n",
    "        response = session.get(variable_link)\n",
    "        response.raise_for_status()\n",
    "        variable_data = response.json()\n",
    "        \n",
    "        # Extract the updated data\n",
    "        updated_data = {\n",
    "            \"Attributes\": variable_data.get('attributes', 'na'),\n",
    "            \"Attribute Of\": variable_data.get('attribute of', 'na'),\n",
    "            \"Attribute Type\": variable_data.get('attribute type', 'na')\n",
    "        }\n",
    "        \n",
    "        # Retrieve the corresponding row from the main table\n",
    "        if index in main_df.index:\n",
    "            main_table_link = main_df.at[index, 'Variable Link']\n",
    "            if variable_link == main_table_link:\n",
    "                # Update the main table with new data\n",
    "                main_df.at[index, 'Attributes'] = updated_data['Attributes']\n",
    "                main_df.at[index, 'Attribute Of'] = updated_data['Attribute Of']\n",
    "                main_df.at[index, 'Attribute Type'] = updated_data['Attribute Type']\n",
    "            else:\n",
    "                raise ValueError(f\"Variable Link mismatch at index {index}.\")\n",
    "        else:\n",
    "            raise KeyError(f\"Index {index} not found in main table.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Capture and log the error\n",
    "        logging.error(f\"Error processing Variable Link {variable_link} (Index {index}): {e}\")\n",
    "        remaining_errors.append({\n",
    "            \"Index\": index,\n",
    "            \"Variable Name\": variable_name,\n",
    "            \"Variable Link\": variable_link,\n",
    "            \"Error\": str(e)\n",
    "        })\n",
    "\n",
    "# Save the updated main table\n",
    "main_df.to_csv(updated_main_table_path, index=False)\n",
    "print(f\"Updated main table saved to {updated_main_table_path}\")\n",
    "\n",
    "# Save remaining errors\n",
    "if remaining_errors:\n",
    "    remaining_errors_df = pd.DataFrame(remaining_errors)\n",
    "    remaining_errors_df.to_csv(remaining_errors_path, index=False)\n",
    "    print(f\"Remaining errors saved to {remaining_errors_path}\")\n",
    "else:\n",
    "    print(\"No remaining errors!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2e0d6c6d-fa82-4144-80f6-a33bab8d2263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk File</th>\n",
       "      <th>Index</th>\n",
       "      <th>Variable Name</th>\n",
       "      <th>Variable Link</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/data_extraction/GroupNodeChunks/GroupNode_chunk_064.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>B17015_001E</td>\n",
       "      <td>https://api.census.gov/data/2013/acs/acs1/variables/B17015_001E.json</td>\n",
       "      <td>HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2013/acs/acs1/variables/B17015_001E.json (Caused by NameResolutionError(\"&lt;urllib3.connection.HTTPSConnection object at 0x15f8a8890&gt;: Failed to resolve 'api.census.gov' ([Errno 8] nodename nor servname provided, or not known)\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/data_extraction/GroupNodeChunks/GroupNode_chunk_064.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>B17015_001EA</td>\n",
       "      <td>https://api.census.gov/data/2013/acs/acs1/variables/B17015_001EA.json</td>\n",
       "      <td>HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2013/acs/acs1/variables/B17015_001EA.json (Caused by NameResolutionError(\"&lt;urllib3.connection.HTTPSConnection object at 0x15f8a93d0&gt;: Failed to resolve 'api.census.gov' ([Errno 8] nodename nor servname provided, or not known)\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/data_extraction/GroupNodeChunks/GroupNode_chunk_064.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>B17015_001M</td>\n",
       "      <td>https://api.census.gov/data/2013/acs/acs1/variables/B17015_001M.json</td>\n",
       "      <td>HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2013/acs/acs1/variables/B17015_001M.json (Caused by NameResolutionError(\"&lt;urllib3.connection.HTTPSConnection object at 0x15f8aa110&gt;: Failed to resolve 'api.census.gov' ([Errno 8] nodename nor servname provided, or not known)\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/data_extraction/GroupNodeChunks/GroupNode_chunk_064.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>B17015_001MA</td>\n",
       "      <td>https://api.census.gov/data/2013/acs/acs1/variables/B17015_001MA.json</td>\n",
       "      <td>HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2013/acs/acs1/variables/B17015_001MA.json (Caused by NameResolutionError(\"&lt;urllib3.connection.HTTPSConnection object at 0x15f8aae50&gt;: Failed to resolve 'api.census.gov' ([Errno 8] nodename nor servname provided, or not known)\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/data_extraction/GroupNodeChunks/GroupNode_chunk_064.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>B17015_002E</td>\n",
       "      <td>https://api.census.gov/data/2013/acs/acs1/variables/B17015_002E.json</td>\n",
       "      <td>HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2013/acs/acs1/variables/B17015_002E.json (Caused by NameResolutionError(\"&lt;urllib3.connection.HTTPSConnection object at 0x15f8abb90&gt;: Failed to resolve 'api.census.gov' ([Errno 8] nodename nor servname provided, or not known)\"))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        Chunk File  Index  \\\n",
       "0  ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_064.csv      0   \n",
       "1  ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_064.csv      1   \n",
       "2  ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_064.csv      2   \n",
       "3  ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_064.csv      3   \n",
       "4  ../data/data_extraction/GroupNodeChunks/GroupNode_chunk_064.csv      4   \n",
       "\n",
       "  Variable Name  \\\n",
       "0   B17015_001E   \n",
       "1  B17015_001EA   \n",
       "2   B17015_001M   \n",
       "3  B17015_001MA   \n",
       "4   B17015_002E   \n",
       "\n",
       "                                                           Variable Link  \\\n",
       "0   https://api.census.gov/data/2013/acs/acs1/variables/B17015_001E.json   \n",
       "1  https://api.census.gov/data/2013/acs/acs1/variables/B17015_001EA.json   \n",
       "2   https://api.census.gov/data/2013/acs/acs1/variables/B17015_001M.json   \n",
       "3  https://api.census.gov/data/2013/acs/acs1/variables/B17015_001MA.json   \n",
       "4   https://api.census.gov/data/2013/acs/acs1/variables/B17015_002E.json   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                             Error  \n",
       "0   HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2013/acs/acs1/variables/B17015_001E.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15f8a8890>: Failed to resolve 'api.census.gov' ([Errno 8] nodename nor servname provided, or not known)\"))  \n",
       "1  HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2013/acs/acs1/variables/B17015_001EA.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15f8a93d0>: Failed to resolve 'api.census.gov' ([Errno 8] nodename nor servname provided, or not known)\"))  \n",
       "2   HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2013/acs/acs1/variables/B17015_001M.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15f8aa110>: Failed to resolve 'api.census.gov' ([Errno 8] nodename nor servname provided, or not known)\"))  \n",
       "3  HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2013/acs/acs1/variables/B17015_001MA.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15f8aae50>: Failed to resolve 'api.census.gov' ([Errno 8] nodename nor servname provided, or not known)\"))  \n",
       "4   HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2013/acs/acs1/variables/B17015_002E.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x15f8abb90>: Failed to resolve 'api.census.gov' ([Errno 8] nodename nor servname provided, or not known)\"))  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc0300-5115-4ef7-945b-3e97fbd4deb8",
   "metadata": {},
   "source": [
    "## Summary Statistics of all Survey-Group Variables\n",
    "- This function counts the total number of variables across all processed chunk files in a specified directory.\n",
    "- It uses a progress tracker to display real-time progress while iterating through the files.\n",
    "- At the end, it outputs the total number of variables and the number of chunk files processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "35d4583c-a20c-4b02-a88b-80eb12ee15aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Variables: 100%|██████████████████| 134/134 [00:50<00:00,  2.65items/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total variables across 134 chunk files: 7696043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from progress_tracker import track_progress\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the chunk files\n",
    "chunk_directory = \"../data/data_extraction/GroupNodesWithVariables\"\n",
    "chunk_file_prefix = \"Processed_GroupNode_chunk_\"\n",
    "chunk_file_suffix = \".csv\"\n",
    "\n",
    "# Initialize variables\n",
    "total_variables = 0\n",
    "chunk_count = 0\n",
    "\n",
    "# Get a list of chunk files\n",
    "chunk_files = [\n",
    "    f for f in os.listdir(chunk_directory)\n",
    "    if f.startswith(chunk_file_prefix) and f.endswith(chunk_file_suffix)\n",
    "]\n",
    "\n",
    "# Process each chunk file with a progress tracker\n",
    "for file_name in track_progress(chunk_files, description=\"Counting Variables\"):\n",
    "    file_path = os.path.join(chunk_directory, file_name)\n",
    "    \n",
    "    # Read the file and count the rows (excluding header)\n",
    "    chunk_df = pd.read_csv(file_path, low_memory=False)\n",
    "    row_count = len(chunk_df)\n",
    "    total_variables += row_count\n",
    "    chunk_count += 1\n",
    "\n",
    "# Print the total count with number of chunks\n",
    "print(f\"Total variables across {chunk_count} chunk files: {total_variables}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1c42c-bedf-4e98-b7fc-8a30ea5a9a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64c59d-01f5-4be8-ad56-d698e3ba5fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a76c5302-73ce-4d33-9751-9c8abd7ac3e6",
   "metadata": {},
   "source": [
    "## 3.6 Build Table: SurveyVariablesNoGroupNode \n",
    "- The variables link has a table with variables in two catagories: GroupID or N/A\n",
    "- We have built the extensive tables with variables in groupd tied to the surveyID in 3.5 above\n",
    "- We must now gather all data on variable linked to SurveyID with group in the variables table where = N/A\n",
    "\n",
    "**Note on Knowledge Graph Construction**: Surveys will have a group node and a no-group node. Then all variables will be the union of the two groups. We could treat 'no group' as a special kind of group. Haven't decided yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937dc6ad-52e1-42c1-9d60-56cf66a1933f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3b5278f1-a241-461b-8364-952fb0f3f4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Variable List: 100%|██████████| 1648/1648 [20:04<00:00,  1.37items/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 539733 rows successfully!\n",
      "Survey Variable processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from progress_tracker import track_progress\n",
    "import logging\n",
    "\n",
    "# Configure logging for failures\n",
    "logging.basicConfig(\n",
    "    filename=\"processing_failures.log\",\n",
    "    level=logging.ERROR,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "def process_html_to_extract_rows(row):\n",
    "    \"\"\"Fetch HTML and extract rows where Group = N/A, then handle attributes.\"\"\"\n",
    "    variable_list_url = row.get(\"Variable List\")\n",
    "    survey_id = row.get(\"SurveyID\")\n",
    "\n",
    "    if not isinstance(variable_list_url, str) or not variable_list_url.endswith(\".html\"):\n",
    "        logging.error(f\"Invalid or missing URL for SurveyID {survey_id}, URL: {variable_list_url}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Fetch the HTML content\n",
    "        response = requests.get(variable_list_url, timeout=10)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            logging.error(f\"Failed to fetch URL {variable_list_url} for SurveyID {survey_id}, Status: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table_rows = soup.find_all('tr')\n",
    "        extracted_rows = []\n",
    "\n",
    "        for tr in table_rows:\n",
    "            columns = [td.get_text(strip=True) for td in tr.find_all('td')]\n",
    "            if len(columns) == 8:  # Ensure expected number of columns\n",
    "                group = columns[7]\n",
    "                \n",
    "                # Filter: Process only rows where Group == \"N/A\"\n",
    "                if group == \"N/A\":\n",
    "                    attributes = columns[4] if columns[4] else \"N/A\"  # Handle blank attributes\n",
    "                    extracted_rows.append({\n",
    "                        \"SurveyID\": survey_id,\n",
    "                        \"Variable Name\": columns[0],\n",
    "                        \"Label\": columns[1],\n",
    "                        \"Concept\": columns[2],\n",
    "                        \"Required\": columns[3],\n",
    "                        \"Attributes\": attributes,\n",
    "                        \"Limit\": columns[5],\n",
    "                        \"Predicate Type\": columns[6],\n",
    "                        \"Group\": group\n",
    "                    })\n",
    "        return extracted_rows\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing URL {variable_list_url} for SurveyID {survey_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_survey_variables(df):\n",
    "    \"\"\"Process the Variable List column and extract SurveyVariablesNoGroupNode.\"\"\"\n",
    "    no_group_nodes = []\n",
    "\n",
    "    for _, row in track_progress(df.iterrows(), description=\"Processing Variable List\", total=len(df)):\n",
    "        extracted_rows = process_html_to_extract_rows(row)\n",
    "        no_group_nodes.extend(extracted_rows)\n",
    "\n",
    "    return no_group_nodes\n",
    "\n",
    "# Main Script\n",
    "survey_df = pd.read_csv(\"../data/data_extraction/SurveyNode-ex-srt-geo-grp.csv\")\n",
    "no_group_nodes = process_survey_variables(survey_df)\n",
    "\n",
    "# Save the SurveyVariablesNoGroupNode table\n",
    "if no_group_nodes:\n",
    "    no_group_df = pd.DataFrame(no_group_nodes)\n",
    "    no_group_df.to_csv(\"../data/data_extraction/SurveyVariablesNoGroupNode.csv\", index=False)\n",
    "    print(f\"Processed {len(no_group_nodes)} rows successfully!\")\n",
    "else:\n",
    "    print(\"No rows were processed successfully.\")\n",
    "\n",
    "print(\"Survey Variable processing completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d0ee3-1429-4042-9d38-24310794cf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8b5a7c8a-48b4-4699-9c8d-8ce82435b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd2113c-41dd-4f66-a116-8aa02c08dd76",
   "metadata": {},
   "source": [
    "# Other file summary stats stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a2ee4599-ed59-4a4e-8428-17b7681ae9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/data_extraction/SurveyNode.csv: 1648 variables\n",
      "../data/data_extraction/SurveyGroupNode.csv: 64873 variables\n",
      "../data/data_extraction/SurveyVariablesNoGroupNode.csv: 539733 variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/yqb3d76d7053f7dyy9fwtkzw0000gp/T/ipykernel_62452/2401853465.py:17: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# Count variables in a file (aka number of data rows)\n",
    "import pandas as pd\n",
    "\n",
    "def count_variables_in_files(file_paths):\n",
    "    \"\"\"\n",
    "    Counts the number of variables (excluding headers) for each file in the list.\n",
    "    \n",
    "    Args:\n",
    "        file_paths (list): List of file paths to process.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with file paths as keys and variable counts as values.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            counts[file_path] = len(df)\n",
    "        except Exception as e:\n",
    "            counts[file_path] = f\"Error: {e}\"\n",
    "    return counts\n",
    "\n",
    "file_paths = [\n",
    "    \"../data/data_extraction/SurveyNode.csv\",\n",
    "    \"../data/data_extraction/SurveyGroupNode.csv\",\n",
    "    \"../data/data_extraction/SurveyVariablesNoGroupNode.csv\"\n",
    "]\n",
    "\n",
    "variable_counts = count_variables_in_files(file_paths)\n",
    "for file, count in variable_counts.items():\n",
    "    print(f\"{file}: {count} variables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f4d37-1291-4db2-b29d-270c0d250baf",
   "metadata": {},
   "source": [
    "# 3. Clean the Data\n",
    "\n",
    "## **Data Structuring and Relationship Design**\n",
    "\n",
    "### **Overview**\n",
    "The goal of structuring the data is to create a hierarchical and navigable knowledge graph that balances usability and precision. This involves grouping repeated surveys into logical parent-child relationships while maintaining links to their associated variables and years.\n",
    "\n",
    "### **Key Design Decisions**\n",
    "1. **Parent-Child Relationships for Datasets:**\n",
    "   - **Why:** Many surveys are conducted multiple times per year, with variations in their metadata. A parent-child hierarchy simplifies navigation for users querying high-level information while retaining granularity for specific queries.\n",
    "   - **How:** We use the `parent_dataset` field to represent the high-level grouping (e.g., `cps` for the Current Population Survey) and `dataset_name` for specific instances (e.g., `cps/basic/jan` for the January survey).\n",
    "\n",
    "2. **Linking Variables to Child Datasets:**\n",
    "   - **Why:** Each dataset instance includes specific variables. Establishing this connection allows users to query datasets for their variables or find which datasets a variable belongs to.\n",
    "   - **How:** We create `Variable` nodes linked to `ChildDataset` nodes via an `INCLUDES` relationship.\n",
    "\n",
    "3. **Year-Based Relationships:**\n",
    "   - **Why:** Many datasets are time-specific. Linking datasets to their respective years ensures queries can filter datasets by year and handle temporal questions like, \"What data is available for 1986?\"\n",
    "   - **How:** We create `Year` nodes and connect them to `ChildDataset` nodes via a `BELONGS_TO_YEAR` relationship.\n",
    "\n",
    "### **Graph Schema**\n",
    "Here is the schema we use to represent the relationships:\n",
    "- **ParentDataset**: Represents high-level groupings of surveys (e.g., `cps`, `cbp`).\n",
    "  - **Relationships:**\n",
    "    - `PARENT_OF` → `ChildDataset`\n",
    "- **ChildDataset**: Represents individual survey instances (e.g., `cps/basic/jan`).\n",
    "  - **Relationships:**\n",
    "    - `INCLUDES` → `Variable`\n",
    "    - `BELONGS_TO_YEAR` → `Year`\n",
    "- **Variable**: Represents specific data variables (e.g., `employment_status`).\n",
    "- **Year**: Represents the temporal context for datasets (e.g., `1986`).\n",
    "\n",
    "### **Why This Structure?**\n",
    "This design ensures:\n",
    "- **Scalability**: Easily add new datasets, variables, and years.\n",
    "- **Usability**: Queries can target high-level overviews or specific details.\n",
    "- **Flexibility**: Supports both general and granular user queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ab13d31c-bed4-4936-99a1-5ede62dba6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading datasets_metadata and variables_metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/yqb3d76d7053f7dyy9fwtkzw0000gp/T/ipykernel_97248/3101690510.py:49: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  variables_metadata = pd.read_csv('./data/combined_variables_metadata.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata loaded.\n",
      "Survey Nodes: 298\n",
      "Dataset Nodes: 3954\n",
      "Variable Nodes: 2693973\n",
      "Survey-to-Dataset Relationships: 3954\n",
      "Dataset-to-Variable Relationships: 2693973\n",
      "Debug files saved:\n",
      "- survey_nodes_debug.csv\n",
      "- dataset_nodes_debug.csv\n",
      "- variable_nodes_debug.csv\n",
      "- survey_to_dataset_relationships_debug.csv\n",
      "- dataset_to_variable_relationships_debug.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def restructure_for_graph(datasets_metadata, variables_metadata):\n",
    "    \"\"\"\n",
    "    Restructure datasets and variables metadata for hierarchical knowledge graph.\n",
    "    Args:\n",
    "        datasets_metadata (pd.DataFrame): Metadata for datasets.\n",
    "        variables_metadata (pd.DataFrame): Metadata for variables.\n",
    "    Returns:\n",
    "        dict: Nodes and relationships for building the graph.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract unique surveys\n",
    "    surveys = datasets_metadata['dataset_name'].str.split('/').str[0].unique()\n",
    "    survey_nodes = pd.DataFrame({'survey': surveys})\n",
    "    print(f\"Survey Nodes: {survey_nodes.shape[0]}\")\n",
    "\n",
    "    # Step 2: Create dataset nodes\n",
    "    datasets_metadata['parent_survey'] = datasets_metadata['dataset_name'].str.split('/').str[0]\n",
    "    datasets_metadata['month'] = datasets_metadata['title'].str.extract(r'(\\bJan|\\bFeb|\\bMar|\\bApr|\\bMay|\\bJun|\\bJul|\\bAug|\\bSep|\\bOct|\\bNov|\\bDec)', expand=False)\n",
    "\n",
    "    dataset_nodes = datasets_metadata[['parent_survey', 'year', 'month', 'title', 'description']].drop_duplicates()\n",
    "    dataset_nodes['dataset_id'] = dataset_nodes.apply(lambda x: f\"{x['parent_survey']}_{x['year']}_{x['month'] or 'Annual'}\", axis=1)\n",
    "    print(f\"Dataset Nodes: {dataset_nodes.shape[0]}\")\n",
    "\n",
    "    # Step 3: Create variable nodes\n",
    "    variables_metadata['parent_survey'] = variables_metadata['dataset_name'].str.split('/').str[0]\n",
    "    variable_nodes = variables_metadata[['parent_survey', 'dataset_name', 'year', 'variable_name', 'label', 'concept']].drop_duplicates()\n",
    "    print(f\"Variable Nodes: {variable_nodes.shape[0]}\")\n",
    "\n",
    "    # Step 4: Create relationships\n",
    "    survey_to_dataset = dataset_nodes[['parent_survey', 'dataset_id']]\n",
    "    dataset_to_variable = variable_nodes[['dataset_name', 'variable_name']]\n",
    "    print(f\"Survey-to-Dataset Relationships: {survey_to_dataset.shape[0]}\")\n",
    "    print(f\"Dataset-to-Variable Relationships: {dataset_to_variable.shape[0]}\")\n",
    "\n",
    "    return {\n",
    "        'survey_nodes': survey_nodes,\n",
    "        'dataset_nodes': dataset_nodes,\n",
    "        'variable_nodes': variable_nodes,\n",
    "        'relationships': {\n",
    "            'survey_to_dataset': survey_to_dataset,\n",
    "            'dataset_to_variable': dataset_to_variable\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Reload and clean metadata\n",
    "print(\"Reloading datasets_metadata and variables_metadata...\")\n",
    "datasets_metadata = pd.read_csv('./data/census_datasets_metadata.csv')\n",
    "variables_metadata = pd.read_csv('./data/combined_variables_metadata.csv')\n",
    "print(\"Metadata loaded.\")\n",
    "\n",
    "# Restructure for graph\n",
    "graph_data = restructure_for_graph(datasets_metadata, variables_metadata)\n",
    "\n",
    "# Extract nodes and relationships\n",
    "survey_nodes = graph_data['survey_nodes']\n",
    "dataset_nodes = graph_data['dataset_nodes']\n",
    "variable_nodes = graph_data['variable_nodes']\n",
    "survey_to_dataset_relationships = graph_data['relationships']['survey_to_dataset']\n",
    "dataset_to_variable_relationships = graph_data['relationships']['dataset_to_variable']\n",
    "\n",
    "# Save to CSV (optional)\n",
    "survey_nodes.to_csv('./data/survey_nodes_debug.csv', index=False)\n",
    "dataset_nodes.to_csv('./data/dataset_nodes_debug.csv', index=False)\n",
    "variable_nodes.to_csv('./data/variable_nodes_debug.csv', index=False)\n",
    "survey_to_dataset_relationships.to_csv('./data/survey_to_dataset_relationships_debug.csv', index=False)\n",
    "dataset_to_variable_relationships.to_csv('./data/dataset_to_variable_relationships_debug.csv', index=False)\n",
    "\n",
    "print(\"Debug files saved:\")\n",
    "print(\"- survey_nodes_debug.csv\")\n",
    "print(\"- dataset_nodes_debug.csv\")\n",
    "print(\"- variable_nodes_debug.csv\")\n",
    "print(\"- survey_to_dataset_relationships_debug.csv\")\n",
    "print(\"- dataset_to_variable_relationships_debug.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d6d11243-f185-4e50-851c-7e77101cc5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Surveys:\n",
      "cps\n",
      "basic\n",
      "jun\n",
      "cbp\n",
      "zbp\n",
      "mar\n",
      "apr\n",
      "pep\n",
      "int_charagegroups\n",
      "aug\n",
      "int_natcivpop\n",
      "int_natresafo\n",
      "dec\n",
      "int_natrespop\n",
      "may\n",
      "ewks\n",
      "jan\n",
      "jul\n",
      "feb\n",
      "nov\n",
      "oct\n",
      "sep\n",
      "nonemp\n",
      "int_charage\n",
      "int_housingunits\n",
      "int_natmonthly\n",
      "int_population\n",
      "surname\n",
      "acs\n",
      "acs1\n",
      "cprofile\n",
      "pums\n",
      "acs5\n",
      "plnat\n",
      "profile\n",
      "subject\n",
      "flows\n",
      "cd113\n",
      "cd115\n",
      "sf1\n",
      "sf2\n",
      "ecnbridge2\n",
      "ecnadmben\n",
      "ecnbranddeal\n",
      "ecnbridge1\n",
      "ecncashadv\n",
      "ecnbrordeal\n",
      "ecnccard\n",
      "ecninvval\n",
      "ecnclcust\n",
      "ecnipa\n",
      "ecncomm\n",
      "ecnkob\n",
      "ecncomp\n",
      "ecnlabor\n",
      "ecnfran\n",
      "ecnconact\n",
      "ecnconcess\n",
      "ecnlifomfg\n",
      "ecncrfin\n",
      "ecnlifomine\n",
      "ecngrant\n",
      "ecndissmed\n",
      "ecnempfunc\n",
      "ecnlifoval\n",
      "ecnentsup\n",
      "ecnlines\n",
      "ecnguest\n",
      "ecneoyinv\n",
      "ecneoyinvwh\n",
      "ecnloan\n",
      "ecnequip\n",
      "ecnlocmfg\n",
      "ecnguestsize\n",
      "ecnexpnrg\n",
      "ecnexpsvc\n",
      "ecnlocmine\n",
      "ecnflspace\n",
      "ecnmargin\n",
      "ecnfoodsvc\n",
      "ecnhosp\n",
      "ecnmatfuel\n",
      "ecnmealcost\n",
      "ecnmenutype\n",
      "ecnpatient\n",
      "ecnpetrfac\n",
      "ecnpetrprod\n",
      "ecnpetrrec\n",
      "ecnpetrstat\n",
      "ecnprofit\n",
      "ecnpurelec\n",
      "ecnpurmode\n",
      "ecnrdacq\n",
      "ecnrdofc\n",
      "ecnseat\n",
      "ecnsize\n",
      "ecnsocial\n",
      "ecntype\n",
      "ecntypop\n",
      "ecnvalcon\n",
      "sbo\n",
      "cscbo\n",
      "popproj\n",
      "births\n",
      "deaths\n",
      "nim\n",
      "pop\n",
      "pubschlfin\n",
      "cre\n",
      "cscb\n",
      "language\n",
      "cochar5\n",
      "cochar6\n",
      "cty\n",
      "housing\n",
      "monthlynatchar5\n",
      "monthlynatchar6\n",
      "natstprc18\n",
      "natstprc\n",
      "prcagesex\n",
      "prm\n",
      "prmagesex\n",
      "stchar5\n",
      "stchar6\n",
      "subcty\n",
      "acsse\n",
      "ase\n",
      "csa\n",
      "asec\n",
      "agesex\n",
      "intltrade\n",
      "imp_exp\n",
      "agespecial5\n",
      "agespecial6\n",
      "agespecialpr\n",
      "projnim\n",
      "projagegroups\n",
      "projpop\n",
      "projbirths\n",
      "projdeaths\n",
      "projnat\n",
      "pdb\n",
      "blockgroup\n",
      "tract\n",
      "charage\n",
      "charagegroups\n",
      "components\n",
      "natmonthly\n",
      "population\n",
      "spp\n",
      "cfsarea\n",
      "cd116\n",
      "ecnbasic\n",
      "pumspr\n",
      "cfsexport\n",
      "cfshazmat\n",
      "cfsprelim\n",
      "ecn\n",
      "islandareas\n",
      "acs3\n",
      "comp\n",
      "napcs\n",
      "agegroups\n",
      "nat\n",
      "responserate\n",
      "pl\n",
      "aian\n",
      "ind\n",
      "lines\n",
      "statecounty\n",
      "disability\n",
      "abscb\n",
      "abscbo\n",
      "abscs\n",
      "tobacco\n",
      "asyoe\n",
      "guyoe\n",
      "mpyoe\n",
      "as\n",
      "mp\n",
      "vi\n",
      "sptprofile\n",
      "gu\n",
      "cfstemp\n",
      "school\n",
      "foodsec\n",
      "aianprofile\n",
      "spt\n",
      "internet\n",
      "vets\n",
      "cd110h\n",
      "volunteer\n",
      "dwjt\n",
      "unbank\n",
      "cd110hprofile\n",
      "sf3profile\n",
      "sldhprofile\n",
      "sldsprofile\n",
      "cd113profile\n",
      "sf2profile\n",
      "ecnnapcsind\n",
      "ecnnapcsprd\n",
      "sf3\n",
      "cd115profile\n",
      "sf4profile\n",
      "sf4\n",
      "sldh\n",
      "slds\n",
      "ecnloccons\n",
      "cd110s\n",
      "arts\n",
      "abstcb\n",
      "cd110sprofile\n",
      "cqr\n",
      "ecnpurgas\n",
      "voting\n",
      "ecninstr\n",
      "fertility\n",
      "ecndirprem\n",
      "ecnadbnprop\n",
      "ecnelmenu\n",
      "ecnhotel\n",
      "ecntypepayer\n",
      "marital\n",
      "immigration\n",
      "cs\n",
      "pubarts\n",
      "absnesd\n",
      "absnesdo\n",
      "eeo\n",
      "pes\n",
      "civic\n",
      "dpas\n",
      "dpgu\n",
      "dpmp\n",
      "dpvi\n",
      "absmcb\n",
      "contworker\n",
      "race\n",
      "library\n",
      "worksched\n",
      "dhc\n",
      "dp\n",
      "dhcas\n",
      "dhcgu\n",
      "dhcmp\n",
      "dhcvi\n",
      "cd118\n",
      "ddhca\n",
      "viusa\n",
      "viusb\n",
      "viusc\n",
      "viusd\n",
      "viuse\n",
      "viusf\n",
      "sipp\n",
      "core\n",
      "2008panel\n",
      "wave14\n",
      "wave10\n",
      "wave16\n",
      "wave1\n",
      "wave11\n",
      "wave15\n",
      "wave12\n",
      "wave2\n",
      "wave13\n",
      "wave3\n",
      "wave4\n",
      "wave5\n",
      "wave6\n",
      "wave7\n",
      "wave8\n",
      "wave9\n",
      "topical\n",
      "2004panel\n",
      "2001panel\n",
      "topicalres\n",
      "crosstabas\n",
      "crosstabgu\n",
      "crosstabmp\n",
      "crosstabvi\n",
      "1996panel\n",
      "1993panel\n",
      "topicaled\n",
      "topicaledex\n",
      "topicalex\n",
      "1992panel\n",
      "1990panel\n",
      "benefit\n",
      "1991panel\n",
      "viuspuf\n",
      "crepuertorico\n",
      "selfresponserate\n",
      "ddhcb\n",
      "rhfs\n",
      "geoinfo\n",
      "sdhc\n",
      "\n",
      "Survey Counts:\n",
      "dataset_name\n",
      "cps             667\n",
      "basic           428\n",
      "acs             246\n",
      "sipp            176\n",
      "dec             122\n",
      "               ... \n",
      "sf4profile        1\n",
      "cd115profile      1\n",
      "sf3               1\n",
      "ecnnapcsprd       1\n",
      "sdhc              1\n",
      "Name: count, Length: 298, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Unique top-level survey names\n",
    "unique_surveys = datasets_metadata['dataset_name'].str.split('/').str[0].unique()\n",
    "print(\"Unique Surveys:\")\n",
    "for survey in unique_surveys:\n",
    "    print(survey)\n",
    "\n",
    "# Count occurrences of each survey\n",
    "survey_counts = datasets_metadata['dataset_name'].str.split('/').str[0].value_counts()\n",
    "print(\"\\nSurvey Counts:\")\n",
    "print(survey_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773504dc-3553-4809-82c3-086c0daade1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cd48ad5b-af68-4a40-8bb1-e18ae4f71119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining Surveys After Filtering:\n",
      "['cps' 'cbp' 'zbp' 'pep' 'int_charagegroups' 'int_natcivpop'\n",
      " 'int_natresafo' 'int_natrespop' 'ewks' 'nonemp' 'int_charage'\n",
      " 'int_housingunits' 'int_natmonthly' 'int_population' 'surname' 'acs'\n",
      " 'acs1' 'cprofile' 'pums' 'acs5' 'plnat' 'profile' 'subject' 'flows'\n",
      " 'cd113' 'cd115' 'sf1' 'sf2' 'ecnbridge2' 'ecnadmben' 'ecnbranddeal'\n",
      " 'ecnbridge1' 'ecncashadv' 'ecnbrordeal' 'ecnccard' 'ecninvval'\n",
      " 'ecnclcust' 'ecnipa' 'ecncomm' 'ecnkob' 'ecncomp' 'ecnlabor' 'ecnfran'\n",
      " 'ecnconact' 'ecnconcess' 'ecnlifomfg' 'ecncrfin' 'ecnlifomine' 'ecngrant'\n",
      " 'ecndissmed' 'ecnempfunc' 'ecnlifoval' 'ecnentsup' 'ecnlines' 'ecnguest'\n",
      " 'ecneoyinv' 'ecneoyinvwh' 'ecnloan' 'ecnequip' 'ecnlocmfg' 'ecnguestsize'\n",
      " 'ecnexpnrg' 'ecnexpsvc' 'ecnlocmine' 'ecnflspace' 'ecnmargin'\n",
      " 'ecnfoodsvc' 'ecnhosp' 'ecnmatfuel' 'ecnmealcost' 'ecnmenutype'\n",
      " 'ecnpatient' 'ecnpetrfac' 'ecnpetrprod' 'ecnpetrrec' 'ecnpetrstat'\n",
      " 'ecnprofit' 'ecnpurelec' 'ecnpurmode' 'ecnrdacq' 'ecnrdofc' 'ecnseat'\n",
      " 'ecnsize' 'ecnsocial' 'ecntype' 'ecntypop' 'ecnvalcon' 'sbo' 'cscbo'\n",
      " 'popproj' 'births' 'deaths' 'nim' 'pop' 'pubschlfin' 'cre' 'cscb'\n",
      " 'language' 'cochar5' 'cochar6' 'cty' 'housing' 'monthlynatchar5'\n",
      " 'monthlynatchar6' 'natstprc18' 'natstprc' 'prcagesex' 'prm' 'prmagesex'\n",
      " 'stchar5' 'stchar6' 'subcty' 'acsse' 'ase' 'csa' 'asec' 'agesex'\n",
      " 'intltrade' 'imp_exp' 'agespecial5' 'agespecial6' 'agespecialpr'\n",
      " 'projnim' 'projagegroups' 'projpop' 'projbirths' 'projdeaths' 'projnat'\n",
      " 'pdb' 'blockgroup' 'tract' 'charage' 'charagegroups' 'components'\n",
      " 'natmonthly' 'population' 'spp' 'cfsarea' 'cd116' 'ecnbasic' 'pumspr'\n",
      " 'cfsexport' 'cfshazmat' 'cfsprelim' 'ecn' 'islandareas' 'acs3' 'comp'\n",
      " 'napcs' 'agegroups' 'nat' 'responserate' 'pl' 'aian' 'ind' 'lines'\n",
      " 'statecounty' 'disability' 'abscb' 'abscbo' 'abscs' 'tobacco' 'asyoe'\n",
      " 'guyoe' 'mpyoe' 'as' 'mp' 'vi' 'sptprofile' 'gu' 'cfstemp' 'school'\n",
      " 'foodsec' 'aianprofile' 'spt' 'internet' 'vets' 'cd110h' 'volunteer'\n",
      " 'dwjt' 'unbank' 'cd110hprofile' 'sf3profile' 'sldhprofile' 'sldsprofile'\n",
      " 'cd113profile' 'sf2profile' 'ecnnapcsind' 'ecnnapcsprd' 'sf3'\n",
      " 'cd115profile' 'sf4profile' 'sf4' 'sldh' 'slds' 'ecnloccons' 'cd110s'\n",
      " 'arts' 'abstcb' 'cd110sprofile' 'cqr' 'ecnpurgas' 'voting' 'ecninstr'\n",
      " 'fertility' 'ecndirprem' 'ecnadbnprop' 'ecnelmenu' 'ecnhotel'\n",
      " 'ecntypepayer' 'marital' 'immigration' 'cs' 'pubarts' 'absnesd'\n",
      " 'absnesdo' 'eeo' 'pes' 'civic' 'dpas' 'dpgu' 'dpmp' 'dpvi' 'absmcb'\n",
      " 'contworker' 'race' 'library' 'worksched' 'dhc' 'dp' 'dhcas' 'dhcgu'\n",
      " 'dhcmp' 'dhcvi' 'cd118' 'ddhca' 'viusa' 'viusb' 'viusc' 'viusd' 'viuse'\n",
      " 'viusf' 'sipp' 'core' '2008panel' 'wave14' 'wave10' 'wave16' 'wave1'\n",
      " 'wave11' 'wave15' 'wave12' 'wave2' 'wave13' 'wave3' 'wave4' 'wave5'\n",
      " 'wave6' 'wave7' 'wave8' 'wave9' 'topical' '2004panel' '2001panel'\n",
      " 'topicalres' 'crosstabas' 'crosstabgu' 'crosstabmp' 'crosstabvi'\n",
      " '1996panel' '1993panel' 'topicaled' 'topicaledex' 'topicalex' '1992panel'\n",
      " '1990panel' 'benefit' '1991panel' 'viuspuf' 'crepuertorico'\n",
      " 'selfresponserate' 'ddhcb' 'rhfs' 'geoinfo' 'sdhc']\n",
      "\n",
      "Filtered datasets saved to './data/cleaned_datasets_metadata.csv'\n"
     ]
    }
   ],
   "source": [
    "# Define surveys to remove\n",
    "surveys_to_remove = [\n",
    "    'basic', 'jan', 'feb', 'mar', 'apr', 'may', \n",
    "    'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'\n",
    "]\n",
    "\n",
    "# Filter out the redundant entries\n",
    "datasets_metadata = datasets_metadata[\n",
    "    ~datasets_metadata['dataset_name'].str.split('/').str[0].isin(surveys_to_remove)\n",
    "]\n",
    "\n",
    "# Recheck the unique surveys after removal\n",
    "remaining_surveys = datasets_metadata['dataset_name'].str.split('/').str[0].unique()\n",
    "print(\"\\nRemaining Surveys After Filtering:\")\n",
    "print(remaining_surveys)\n",
    "\n",
    "# Save the cleaned dataset for further inspection\n",
    "datasets_metadata.to_csv('./data/cleaned_datasets_metadata.csv', index=False)\n",
    "print(\"\\nFiltered datasets saved to './data/cleaned_datasets_metadata.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c2e78ee3-abdd-4007-bf9c-8d725170e423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Datasets Metadata:\n",
      "  dataset_name  year                                              title  \\\n",
      "0          cps  1994  Jun 1994 Current Population Survey: Basic Monthly   \n",
      "3          cbp  1986   1986 County Business Patterns: Business Patterns   \n",
      "4          zbp  1994  1994 County Business Patterns - Zip Code Busin...   \n",
      "5          cbp  1987   1987 County Business Patterns: Business Patterns   \n",
      "6          cbp  1995   1995 County Business Patterns: Business Patterns   \n",
      "\n",
      "                                         description  \\\n",
      "0  To provide estimates of employment, unemployme...   \n",
      "3  County Business Patterns (CBP) is an annual se...   \n",
      "4  ZIP Code Business Patterns (ZBP) is an annual ...   \n",
      "5  County Business Patterns (CBP) is an annual se...   \n",
      "6  County Business Patterns (CBP) is an annual se...   \n",
      "\n",
      "                                      identifier    contact access_level  \\\n",
      "0  https://api.census.gov/data/id/CPSBASIC199406  CPS Staff       public   \n",
      "3          http://api.census.gov/data/id/CBP1986  CBP Staff       public   \n",
      "4     http://api.census.gov/data/id/ZBPTotal1994  CBP Staff       public   \n",
      "5          http://api.census.gov/data/id/CBP1987  CBP Staff       public   \n",
      "6          http://api.census.gov/data/id/CBP1995  CBP Staff       public   \n",
      "\n",
      "                modified           publisher  \\\n",
      "0  2019-10-09 15:05:36.0  U.S. Census Bureau   \n",
      "3             2019-02-13  U.S. Census Bureau   \n",
      "4             2018-01-25  U.S. Census Bureau   \n",
      "5             2019-02-13  U.S. Census Bureau   \n",
      "6             2019-02-13  U.S. Census Bureau   \n",
      "\n",
      "                           references keywords parent_survey month  \n",
      "0  https://www.census.gov/developers/   census           cps   Jun  \n",
      "3   http://www.census.gov/developers/   census           cbp   NaN  \n",
      "4   http://www.census.gov/developers/   census           zbp   NaN  \n",
      "5   http://www.census.gov/developers/   census           cbp   NaN  \n",
      "6   http://www.census.gov/developers/   census           cbp   NaN  \n",
      "\n",
      "Filtered datasets saved to './data/cleaned_datasets_metadata.csv'\n"
     ]
    }
   ],
   "source": [
    "# Ensure `parent_survey` column exists in the datasets_metadata\n",
    "datasets_metadata['parent_survey'] = datasets_metadata['dataset_name'].str.split('/').str[0]\n",
    "\n",
    "# Filter rows where parent_survey equals dataset_name\n",
    "datasets_metadata = datasets_metadata[datasets_metadata['parent_survey'] == datasets_metadata['dataset_name']]\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(\"\\nFiltered Datasets Metadata:\")\n",
    "print(datasets_metadata.head())\n",
    "\n",
    "# Save the filtered dataset to a new file for further inspection\n",
    "datasets_metadata.to_csv('./data/cleaned_datasets_metadata.csv', index=False)\n",
    "print(\"\\nFiltered datasets saved to './data/cleaned_datasets_metadata.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7492698-2d15-4e3b-9556-5423253d913b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a782641-326e-4c08-b568-814dbc108408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a26fd-20ef-460c-a6fa-8eeb0920be7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7497dc-b41c-47a8-80f0-37b0115c89be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9574e7-382b-4253-b966-191df3071f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56e4ec-00c3-4d3f-b88a-36ae609d9b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78d5c5b9-4308-47c9-820f-6fc24d120a8f",
   "metadata": {},
   "source": [
    "# 4. Connect to Neo4j\n",
    "- This cell sets up the Neo4j connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c613301-40ee-4d0c-a216-a8c9e9e6f377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Initialize Neo4j connection (replace with your credentials)\n",
    "neo4j_uri = \"bolt://localhost:7687\"  # Update with your Neo4j URI\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"password\"  # Update with your password\n",
    "\n",
    "try:\n",
    "    # Create a driver instance\n",
    "    driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "    \n",
    "    # Test connection by opening a session and executing a simple query\n",
    "    with driver.session() as session:\n",
    "        session.run(\"RETURN 1\")  # Simple query to check connection\n",
    "    print(\"Connection successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06216e2c-9b7a-4908-8bdd-080576506912",
   "metadata": {},
   "source": [
    "# 5. Ingest Data into Neo4j\n",
    "- This cell contains the ingestion logic.\n",
    "\n",
    "## Clear the Neo4J dB if necessary\n",
    "To delete all the data in your Neo4j database, you can use the following Cypher query, which will remove all nodes and relationships: \\\n",
    "> MATCH (n) \\\n",
    "> DETACH DELETE n\n",
    "\n",
    "### Explanation:\n",
    "- MATCH (n): This matches all nodes in the graph.\n",
    "- DETACH DELETE n: This deletes the nodes and any relationships attached to them.\n",
    "\n",
    "### How to Run:\n",
    "1. Open your Neo4j browser or a Neo4j client.\n",
    "1. Paste the query and execute it.\n",
    "\n",
    "This will completely clear your Neo4j database of all nodes and relationships, giving you a fresh starting point for your new data ingestion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d3d45ac-964e-4560-a4db-010207c6896b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataFrames...\n",
      "DataFrames created. Starting Neo4j loading process...\n",
      "Clearing existing data...\n",
      "Existing data cleared.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Survey Nodes: 100%|██████████████████| 298/298 [00:02<00:00, 109.76it/s]\n",
      "Loading Dataset Nodes: 100%|████████████████| 3954/3954 [00:42<00:00, 94.01it/s]\n",
      "Loading Variable Nodes:   0%|         | 7000/2693973 [01:03<6:44:00, 110.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 70\u001b[0m\n\u001b[1;32m     59\u001b[0m load_nodes_in_batches(\n\u001b[1;32m     60\u001b[0m     driver,\n\u001b[1;32m     61\u001b[0m     dataset_nodes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Dataset Nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Load Variable nodes\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[43mload_nodes_in_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;43;03m    MERGE (v:Variable {variable_name: $variable_name})\u001b[39;49;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;43;03m    SET v.label = $label, v.concept = $concept\u001b[39;49;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading Variable Nodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     78\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Create Survey -> Dataset relationships\u001b[39;00m\n\u001b[1;32m     81\u001b[0m load_relationships_in_batches(\n\u001b[1;32m     82\u001b[0m     driver,\n\u001b[1;32m     83\u001b[0m     survey_to_dataset_relationships,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating Survey -> Dataset Relationships\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m )\n",
      "Cell \u001b[0;32mIn[68], line 28\u001b[0m, in \u001b[0;36mload_nodes_in_batches\u001b[0;34m(driver, df, query, desc, batch_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m driver\u001b[38;5;241m.\u001b[39msession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 28\u001b[0m         \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(batch))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.11/site-packages/neo4j/_sync/work/session.py:303\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientError(\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplicit Transaction must be handled explicitly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_result:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# This will buffer upp all records for the previous auto-commit tx\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mdefault_access_mode)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.11/site-packages/neo4j/_sync/work/result.py:454\u001b[0m, in \u001b[0;36mResult._buffer_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_all\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.11/site-packages/neo4j/_sync/work/result.py:443\u001b[0m, in \u001b[0;36mResult._buffer\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    442\u001b[0m record_buffer \u001b[38;5;241m=\u001b[39m deque()\n\u001b[0;32m--> 443\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecord_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrecord_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.11/site-packages/neo4j/_sync/work/result.py:393\u001b[0m, in \u001b[0;36mResult.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_buffer\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discarding:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discard()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:181\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:974\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[0;32m--> 974\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhydration_hooks\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_message(tag, fields)\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:74\u001b[0m, in \u001b[0;36mInbox.pop\u001b[0;34m(self, hydration_hooks)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, hydration_hooks):\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_one_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         size, tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unpacker\u001b[38;5;241m.\u001b[39munpack_structure_header()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:51\u001b[0m, in \u001b[0;36mInbox._buffer_one_chunk\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m chunk_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;66;03m# Determine the chunk size and skip noop\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m         \u001b[43mreceive_into_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m         chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mpop_u16()\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:336\u001b[0m, in \u001b[0;36mreceive_into_buffer\u001b[0;34m(sock, buffer, n_bytes)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(buffer\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;28;01mas\u001b[39;00m view:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mused \u001b[38;5;241m<\u001b[39m end:\n\u001b[0;32m--> 336\u001b[0m         n \u001b[38;5;241m=\u001b[39m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m            \u001b[49m\u001b[43mview\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mused\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mused\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    340\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.11/site-packages/neo4j/_async_compat/network/_bolt_socket.py:521\u001b[0m, in \u001b[0;36mBoltSocket.recv_into\u001b[0;34m(self, buffer, nbytes)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecv_into\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer, nbytes):\n\u001b[0;32m--> 521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.11/site-packages/neo4j/_async_compat/network/_bolt_socket.py:496\u001b[0m, in \u001b[0;36mBoltSocket._wait_for_io\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait_for_io\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39mgettimeout()\n\u001b[1;32m    498\u001b[0m     deadline_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deadline\u001b[38;5;241m.\u001b[39mto_timeout()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Neo4j connection\n",
    "neo4j_uri = \"bolt://localhost:7687\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"password\"\n",
    "driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "\n",
    "# Load data from CSV files\n",
    "print(\"Creating DataFrames...\")\n",
    "survey_nodes = pd.read_csv('./data/survey_nodes.csv')\n",
    "dataset_nodes = pd.read_csv('./data/dataset_nodes.csv')\n",
    "variable_nodes = pd.read_csv('./data/variable_nodes.csv')\n",
    "survey_to_dataset_relationships = pd.read_csv('./data/survey_to_dataset_relationships.csv')\n",
    "dataset_to_variable_relationships = pd.read_csv('./data/dataset_to_variable_relationships.csv')\n",
    "print(\"DataFrames created. Starting Neo4j loading process...\")\n",
    "\n",
    "# Function to load nodes in batches with progress bar\n",
    "def load_nodes_in_batches(driver, df, query, desc, batch_size=1000):\n",
    "    total_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    with tqdm(total=len(df), desc=desc) as pbar:\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            with driver.session() as session:\n",
    "                for _, row in batch.iterrows():\n",
    "                    session.run(query, **row.to_dict())\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "# Function to load relationships in batches with progress bar\n",
    "def load_relationships_in_batches(driver, df, query, desc, batch_size=1000):\n",
    "    total_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    with tqdm(total=len(df), desc=desc) as pbar:\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            with driver.session() as session:\n",
    "                for _, row in batch.iterrows():\n",
    "                    session.run(query, **row.to_dict())\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "# Clear existing data\n",
    "with driver.session() as session:\n",
    "    print(\"Clearing existing data...\")\n",
    "    session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "print(\"Existing data cleared.\")\n",
    "\n",
    "# Load Survey nodes\n",
    "load_nodes_in_batches(\n",
    "    driver,\n",
    "    survey_nodes,\n",
    "    \"\"\"\n",
    "    MERGE (s:Survey {survey: $survey})\n",
    "    \"\"\",\n",
    "    \"Loading Survey Nodes\"\n",
    ")\n",
    "\n",
    "# Load Dataset nodes\n",
    "load_nodes_in_batches(\n",
    "    driver,\n",
    "    dataset_nodes,\n",
    "    \"\"\"\n",
    "    MERGE (d:Dataset {dataset_id: $dataset_id})\n",
    "    SET d.year = $year, d.month = $month, d.title = $title, d.description = $description\n",
    "    \"\"\",\n",
    "    \"Loading Dataset Nodes\"\n",
    ")\n",
    "\n",
    "# Load Variable nodes\n",
    "load_nodes_in_batches(\n",
    "    driver,\n",
    "    variable_nodes,\n",
    "    \"\"\"\n",
    "    MERGE (v:Variable {variable_name: $variable_name})\n",
    "    SET v.label = $label, v.concept = $concept\n",
    "    \"\"\",\n",
    "    \"Loading Variable Nodes\"\n",
    ")\n",
    "\n",
    "# Create Survey -> Dataset relationships\n",
    "load_relationships_in_batches(\n",
    "    driver,\n",
    "    survey_to_dataset_relationships,\n",
    "    \"\"\"\n",
    "    MATCH (s:Survey {survey: $parent_survey})\n",
    "    MATCH (d:Dataset {dataset_id: $dataset_id})\n",
    "    MERGE (s)-[:HAS_DATASET]->(d)\n",
    "    \"\"\",\n",
    "    \"Creating Survey -> Dataset Relationships\"\n",
    ")\n",
    "\n",
    "# Create Dataset -> Variable relationships\n",
    "load_relationships_in_batches(\n",
    "    driver,\n",
    "    dataset_to_variable_relationships,\n",
    "    \"\"\"\n",
    "    MATCH (d:Dataset {dataset_id: $dataset_name})\n",
    "    MATCH (v:Variable {variable_name: $variable_name})\n",
    "    MERGE (d)-[:HAS_VARIABLE]->(v)\n",
    "    \"\"\",\n",
    "    \"Creating Dataset -> Variable Relationships\"\n",
    ")\n",
    "\n",
    "print(\"All data loaded into Neo4j successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281dcde-c1cf-4aa1-bf5b-b4abdb70dc28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0f27d-b487-4fe2-a16c-e4ef0b8f8819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39057c8a-2183-4960-a008-56515d3d45bf",
   "metadata": {},
   "source": [
    "# Step 6: Securely Loading OpenAI API Key and Using LLM for Concept Extraction\n",
    "\n",
    "The goal of this step is to securely load the OpenAI API key from a `.env` file and utilize the LLM (Large Language Model) to enhance the knowledge graph. By extracting key concepts and terms from variable descriptions, we enrich the graph with semantic information.\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Loading the API Key**: \n",
    "    - We load the OpenAI API key securely from a `.env` file using the `python-dotenv` package. This avoids hardcoding sensitive credentials in the source code, ensuring better security and flexibility.\n",
    "   \n",
    "2. **Using LLM for Concept Extraction**:\n",
    "    - Once the API key is loaded, we use OpenAI’s GPT-based model to process the variable descriptions. The model extracts key concepts, terms, and entities from the descriptions (e.g., \"income,\" \"education level\"), which we can then use to create new concept nodes in the knowledge graph.\n",
    "   \n",
    "3. **Enhancing the Knowledge Graph**:\n",
    "    - After extracting the concepts, we create **concept nodes** and link them to the relevant **variables** using `:MEASURES` relationships. We can also link surveys that cover similar concepts, allowing us to analyze relationships between datasets based on the concepts they measure.\n",
    "\n",
    "This approach leverages the power of GPT to enhance the graph beyond simple variable names, creating a richer, more semantically aware dataset that will be useful for querying, analysis, and discovering relationships that were not obvious at first glance.\n",
    "\n",
    "## 6.1 Loading the API Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6e1e4-02f9-45fb-9bc0-b6ff6333dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key from the environment\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba57a1-4b0c-40a9-a42a-d7012df8b90c",
   "metadata": {},
   "source": [
    "## 6.2 Testing functionality/connectivity of the GPT\n",
    "\n",
    "### Extracting Key Concepts with ChatGPT\n",
    "This is for testing, to be sure things are working. In this step, we'll define a function that uses OpenAI's GPT model to process variable descriptions and extract important concepts, keywords, or themes. The extracted concepts will help us create concept nodes in the knowledge graph.\n",
    "\n",
    "### Here’s the process:\n",
    "1. Input: We'll pass the variable descriptions to the GPT model via the OpenAI API.\n",
    "1. Output: The model will return key concepts or keywords that are semantically related to the descriptions.\n",
    "1. Linking: These concepts will be used to enrich the knowledge graph, creating concept nodes and linking them to the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca508a3-ef1b-4fa6-991e-11d8e5149596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def extract_concepts_from_description(description):\n",
    "    \"\"\"\n",
    "    Extract key concepts from a description using OpenAI's API\n",
    "    \n",
    "    Args:\n",
    "        description (str): The text description to analyze\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted concepts and terms\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()  # Uses OPENAI_API_KEY from environment\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Can upgrade to gpt-4 if available\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in concept extraction.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Extract key concepts and terms from this variable description: {description}\"}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        # Extract concepts from the response\n",
    "        concepts = response.choices[0].message.content.strip()\n",
    "        \n",
    "        return concepts\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in concept extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_description = \"Flag for Production workers average for year\"\n",
    "    concepts = extract_concepts_from_description(sample_description)\n",
    "    print(\"Extracted Concepts:\", concepts)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40085ad-4456-451e-9934-d6a80bfad5dc",
   "metadata": {},
   "source": [
    "## Step 6.3: Link Variables to Concepts\n",
    "\n",
    "The goal here is to take the concepts extracted from the variable descriptions and link them to their corresponding variables in the Neo4j knowledge graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff23af-36d8-4ea8-b8c3-397e6aab6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "log_file_path = 'concept_extraction.log'\n",
    "# Ensure the log file is cleared each time\n",
    "if os.path.exists(log_file_path):\n",
    "    os.remove(log_file_path)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s: %(message)s'\n",
    ")\n",
    "\n",
    "# Function to extract concepts from the variable description using OpenAI\n",
    "def extract_concepts_from_description(description):\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # You can upgrade to gpt-4 if desired\n",
    "            prompt=f\"Extract key concepts and terms from this variable description: {description}\",\n",
    "            max_tokens=100,  # Limit tokens to get a concise response\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        concepts = response.choices[0].text.strip()\n",
    "        return concepts\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in concept extraction for description '{description}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Main extraction process\n",
    "def extract_missing_concepts(child_to_variable):\n",
    "    logging.info(\"Starting concept extraction for missing entries\")\n",
    "    \n",
    "    for index, row in child_to_variable.iterrows():\n",
    "        if pd.isna(row['concept']) or row['concept'] == '':  # If concept is missing\n",
    "            logging.info(f\"Attempting to extract concept for variable {row['variable_name']}\")\n",
    "            \n",
    "            # Attempt to extract the concept using the variable's label or description\n",
    "            description = row['label']  # Or use another column if description is separate\n",
    "            extracted_concepts = extract_concepts_from_description(description)\n",
    "            \n",
    "            if extracted_concepts:\n",
    "                logging.info(f\"Successfully linked Variable: {row['variable_name']} to Concept: {extracted_concepts}\")\n",
    "                child_to_variable.at[index, 'concept'] = extracted_concepts  # Assign the extracted concept\n",
    "            else:\n",
    "                logging.warning(f\"Failed to extract concept for {row['variable_name']}\")\n",
    "    \n",
    "    logging.info(\"Concept extraction process completed\")\n",
    "    return child_to_variable\n",
    "\n",
    "# Actual execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure OpenAI API key is set\n",
    "    if not os.getenv('OPENAI_API_KEY'):\n",
    "        logging.error(\"OpenAI API key not set. Please set the OPENAI_API_KEY environment variable.\")\n",
    "        raise ValueError(\"OpenAI API key is required\")\n",
    "\n",
    "    # Process all data\n",
    "    updated_child_to_variable = extract_missing_concepts(child_to_variable)\n",
    "    \n",
    "    # Save the updated DataFrame\n",
    "    updated_child_to_variable.to_csv('updated_child_to_variable.csv', index=False)\n",
    "    \n",
    "    # Display updated entries\n",
    "    print(\"Concepts successfully populated for missing entries:\")\n",
    "    print(updated_child_to_variable.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdada55-23d5-4b5c-82c8-4584bb1f8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(child_to_variable[child_to_variable['variable_name'] == 'EMPAVPW_F'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9088c8e-5c1d-4f3a-92f1-349272b61ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (graphrag_env)",
   "language": "python",
   "name": "graphrag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
